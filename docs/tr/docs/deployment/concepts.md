# Deployment KavramlarÄ± { #deployments-concepts }

Bir **FastAPI** uygulamasÄ±nÄ± (hatta genel olarak herhangi bir web API'yi) deploy ederken, muhtemelen Ã¶nemseyeceÄŸiniz bazÄ± kavramlar vardÄ±r. Bu kavramlarÄ± kullanarak, **uygulamanÄ±zÄ± deploy etmek** iÃ§in **en uygun** yÃ¶ntemi bulabilirsiniz.

Ã–nemli kavramlardan bazÄ±larÄ± ÅŸunlardÄ±r:

* GÃ¼venlik - HTTPS
* Startup'ta Ã§alÄ±ÅŸtÄ±rma
* Yeniden baÅŸlatmalar
* Replikasyon (Ã§alÄ±ÅŸan process sayÄ±sÄ±)
* Bellek
* BaÅŸlatmadan Ã¶nceki adÄ±mlar

BunlarÄ±n **deployment**'larÄ± nasÄ±l etkilediÄŸine bakalÄ±m.

Nihai hedef, **API client**'larÄ±nÄ±za **gÃ¼venli** bir ÅŸekilde hizmet verebilmek, **kesintileri** Ã¶nlemek ve **hesaplama kaynaklarÄ±nÄ±** (Ã¶r. uzak server'lar/sanal makineler) olabildiÄŸince verimli kullanmaktÄ±r. ğŸš€

Burada bu **kavramlar** hakkÄ±nda biraz daha bilgi vereceÄŸim. BÃ¶ylece, Ã§ok farklÄ± ortamlardaâ€”hatta bugÃ¼n var olmayan **gelecekteki** ortamlarda bileâ€”API'nizi nasÄ±l deploy edeceÄŸinize karar verirken ihtiyaÃ§ duyacaÄŸÄ±nÄ±z **sezgiyi** kazanmÄ±ÅŸ olursunuz.

Bu kavramlarÄ± dikkate alarak, **kendi API**'leriniz iÃ§in en iyi deployment yaklaÅŸÄ±mÄ±nÄ± **deÄŸerlendirebilir ve tasarlayabilirsiniz**.

Sonraki bÃ¶lÃ¼mlerde, FastAPI uygulamalarÄ±nÄ± deploy etmek iÃ§in daha **somut tarifler** (recipes) paylaÅŸacaÄŸÄ±m.

Ama ÅŸimdilik, bu Ã¶nemli **kavramsal fikirleri** inceleyelim. Bu kavramlar diÄŸer tÃ¼m web API tÃ¼rleri iÃ§in de geÃ§erlidir. ğŸ’¡

## GÃ¼venlik - HTTPS { #security-https }

[HTTPS hakkÄ±ndaki Ã¶nceki bÃ¶lÃ¼mde](https.md){.internal-link target=_blank} HTTPS'in API'niz iÃ§in nasÄ±l ÅŸifreleme saÄŸladÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenmiÅŸtik.

AyrÄ±ca HTTPS'in genellikle uygulama server'Ä±nÄ±zÄ±n **dÄ±ÅŸÄ±nda** yer alan bir bileÅŸen tarafÄ±ndan saÄŸlandÄ±ÄŸÄ±nÄ±, yani bir **TLS Termination Proxy** ile yapÄ±ldÄ±ÄŸÄ±nÄ± da gÃ¶rmÃ¼ÅŸtÃ¼k.

Ve **HTTPS sertifikalarÄ±nÄ± yenilemekten** sorumlu bir ÅŸey olmalÄ±dÄ±r; bu aynÄ± bileÅŸen olabileceÄŸi gibi farklÄ± bir bileÅŸen de olabilir.

### HTTPS iÃ§in Ã–rnek AraÃ§lar { #example-tools-for-https }

TLS Termination Proxy olarak kullanabileceÄŸiniz bazÄ± araÃ§lar:

* Traefik
    * Sertifika yenilemelerini otomatik yÃ¶netir âœ¨
* Caddy
    * Sertifika yenilemelerini otomatik yÃ¶netir âœ¨
* Nginx
    * Sertifika yenilemeleri iÃ§in Certbot gibi harici bir bileÅŸenle
* HAProxy
    * Sertifika yenilemeleri iÃ§in Certbot gibi harici bir bileÅŸenle
* Nginx gibi bir Ingress Controller ile Kubernetes
    * Sertifika yenilemeleri iÃ§in cert-manager gibi harici bir bileÅŸenle
* Bir cloud provider tarafÄ±ndan servislerinin parÃ§asÄ± olarak iÃ§eride yÃ¶netilmesi (aÅŸaÄŸÄ±yÄ± okuyun ğŸ‘‡)

Bir diÄŸer seÃ§enek de, HTTPS kurulumunu da dahil olmak Ã¼zere iÅŸin daha bÃ¼yÃ¼k kÄ±smÄ±nÄ± yapan bir **cloud service** kullanmaktÄ±r. Bunun bazÄ± kÄ±sÄ±tlarÄ± olabilir veya daha pahalÄ± olabilir vb. Ancak bu durumda TLS Termination Proxy'yi kendiniz kurmak zorunda kalmazsÄ±nÄ±z.

Sonraki bÃ¶lÃ¼mlerde bazÄ± somut Ã¶rnekler gÃ¶stereceÄŸim.

---

Sonraki kavramlar, gerÃ§ek API'nizi Ã§alÄ±ÅŸtÄ±ran programla (Ã¶r. Uvicorn) ilgilidir.

## Program ve Process { #program-and-process }

Ã‡alÄ±ÅŸan "**process**" hakkÄ±nda Ã§ok konuÅŸacaÄŸÄ±z. Bu yÃ¼zden ne anlama geldiÄŸini ve "**program**" kelimesinden farkÄ±nÄ±n ne olduÄŸunu netleÅŸtirmek faydalÄ±.

### Program Nedir { #what-is-a-program }

**Program** kelimesi gÃ¼nlÃ¼k kullanÄ±mda birÃ§ok ÅŸeyi anlatmak iÃ§in kullanÄ±lÄ±r:

* YazdÄ±ÄŸÄ±nÄ±z **code**, yani **Python dosyalarÄ±**.
* Ä°ÅŸletim sistemi tarafÄ±ndan **Ã§alÄ±ÅŸtÄ±rÄ±labilen** **dosya**, Ã¶rn: `python`, `python.exe` veya `uvicorn`.
* Ä°ÅŸletim sistemi Ã¼zerinde **Ã§alÄ±ÅŸÄ±r durumdayken** CPU kullanan ve bellekte veri tutan belirli bir program. Buna **process** de denir.

### Process Nedir { #what-is-a-process }

**Process** kelimesi genellikle daha spesifik kullanÄ±lÄ±r; yalnÄ±zca iÅŸletim sistemi Ã¼zerinde Ã§alÄ±ÅŸan ÅŸeye (yukarÄ±daki son madde gibi) iÅŸaret eder:

* Ä°ÅŸletim sistemi Ã¼zerinde **Ã§alÄ±ÅŸÄ±r durumda** olan belirli bir program.
    * Bu; dosyayÄ± ya da code'u deÄŸil, iÅŸletim sistemi tarafÄ±ndan **Ã§alÄ±ÅŸtÄ±rÄ±lan** ve yÃ¶netilen ÅŸeyi ifade eder.
* Herhangi bir program, herhangi bir code, **yalnÄ±zca Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rken** bir ÅŸey yapabilir. Yani bir **process Ã§alÄ±ÅŸÄ±yorken**.
* Process siz tarafÄ±ndan veya iÅŸletim sistemi tarafÄ±ndan **sonlandÄ±rÄ±labilir** (ya da "killed" edilebilir). O anda Ã§alÄ±ÅŸmasÄ±/Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± durur ve artÄ±k **hiÃ§bir ÅŸey yapamaz**.
* BilgisayarÄ±nÄ±zda Ã§alÄ±ÅŸan her uygulamanÄ±n arkasÄ±nda bir process vardÄ±r; Ã§alÄ±ÅŸan her program, her pencere vb. Bilgisayar aÃ§Ä±kken normalde **aynÄ± anda** birÃ§ok process Ã§alÄ±ÅŸÄ±r.
* AynÄ± anda **aynÄ± programÄ±n birden fazla process**'i Ã§alÄ±ÅŸabilir.

Ä°ÅŸletim sisteminizdeki "task manager" veya "system monitor" (ya da benzeri araÃ§lar) ile bu process'lerin birÃ§oÄŸunu Ã§alÄ±ÅŸÄ±r halde gÃ¶rebilirsiniz.

Ã–rneÄŸin muhtemelen aynÄ± browser programÄ±nÄ± (Firefox, Chrome, Edge vb.) Ã§alÄ±ÅŸtÄ±ran birden fazla process gÃ¶receksiniz. Genelde her tab iÃ§in bir process, Ã¼stÃ¼ne bazÄ± ek process'ler Ã§alÄ±ÅŸtÄ±rÄ±rlar.

<img class="shadow" src="/img/deployment/concepts/image01.png">

---

ArtÄ±k **process** ve **program** arasÄ±ndaki farkÄ± bildiÄŸimize gÃ¶re, deployment konusuna devam edelim.

## Startup'ta Ã‡alÄ±ÅŸtÄ±rma { #running-on-startup }

Ã‡oÄŸu durumda bir web API oluÅŸturduÄŸunuzda, client'larÄ±nÄ±zÄ±n her zaman eriÅŸebilmesi iÃ§in API'nizin kesintisiz ÅŸekilde **sÃ¼rekli Ã§alÄ±ÅŸÄ±yor** olmasÄ±nÄ± istersiniz. Elbette sadece belirli durumlarda Ã§alÄ±ÅŸmasÄ±nÄ± istemenizin Ã¶zel bir sebebi olabilir; ancak Ã§oÄŸunlukla onu sÃ¼rekli aÃ§Ä±k ve **kullanÄ±labilir** halde tutarsÄ±nÄ±z.

### Uzak Bir Server'da { #in-a-remote-server }

Uzak bir server (cloud server, sanal makine vb.) kurduÄŸunuzda, yapabileceÄŸiniz en basit ÅŸey; local geliÅŸtirme sÄ±rasÄ±nda yaptÄ±ÄŸÄ±nÄ±z gibi, manuel olarak `fastapi run` (Uvicorn'u kullanÄ±r) veya benzeri bir komutla Ã§alÄ±ÅŸtÄ±rmaktÄ±r.

Bu yÃ¶ntem Ã§alÄ±ÅŸÄ±r ve **geliÅŸtirme sÄ±rasÄ±nda** faydalÄ±dÄ±r.

Ancak server'a olan baÄŸlantÄ±nÄ±z koparsa, **Ã§alÄ±ÅŸan process** muhtemelen Ã¶lÃ¼r.

Ve server yeniden baÅŸlatÄ±lÄ±rsa (Ã¶rneÄŸin update'lerden sonra ya da cloud provider'Ä±n migration'larÄ±ndan sonra) bunu muhtemelen **fark etmezsiniz**. DolayÄ±sÄ±yla process'i manuel yeniden baÅŸlatmanÄ±z gerektiÄŸini de bilmezsiniz. SonuÃ§ta API'niz Ã¶lÃ¼ kalÄ±r. ğŸ˜±

### Startup'ta Otomatik Ã‡alÄ±ÅŸtÄ±rma { #run-automatically-on-startup }

Genellikle server programÄ±nÄ±n (Ã¶r. Uvicorn) server aÃ§Ä±lÄ±ÅŸÄ±nda otomatik baÅŸlamasÄ±nÄ± ve herhangi bir **insan mÃ¼dahalesi** gerektirmeden API'nizi Ã§alÄ±ÅŸtÄ±ran bir process'in sÃ¼rekli ayakta olmasÄ±nÄ± istersiniz (Ã¶r. FastAPI uygulamanÄ±zÄ± Ã§alÄ±ÅŸtÄ±ran Uvicorn).

### AyrÄ± Bir Program { #separate-program }

Bunu saÄŸlamak iÃ§in genellikle startup'ta uygulamanÄ±zÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olacak **ayrÄ± bir program** kullanÄ±rsÄ±nÄ±z. Pek Ã§ok durumda bu program, Ã¶rneÄŸin bir veritabanÄ± gibi diÄŸer bileÅŸenlerin/uygulamalarÄ±n da Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olur.

### Startup'ta Ã‡alÄ±ÅŸtÄ±rmak iÃ§in Ã–rnek AraÃ§lar { #example-tools-to-run-at-startup }

Bu iÅŸi yapabilen araÃ§lara Ã¶rnekler:

* Docker
* Kubernetes
* Docker Compose
* Docker in Swarm Mode
* Systemd
* Supervisor
* Bir cloud provider tarafÄ±ndan servislerinin parÃ§asÄ± olarak iÃ§eride yÃ¶netilmesi
* DiÄŸerleri...

Sonraki bÃ¶lÃ¼mlerde daha somut Ã¶rnekler vereceÄŸim.

## Yeniden BaÅŸlatmalar { #restarts }

UygulamanÄ±zÄ±n startup'ta Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olmaya benzer ÅŸekilde, hatalardan sonra **yeniden baÅŸlatÄ±ldÄ±ÄŸÄ±ndan** da emin olmak istersiniz.

### Hata YaparÄ±z { #we-make-mistakes }

Biz insanlar sÃ¼rekli **hata** yaparÄ±z. YazÄ±lÄ±mÄ±n neredeyse *her zaman* farklÄ± yerlerinde gizli **bug**'lar vardÄ±r. ğŸ›

Ve biz geliÅŸtiriciler bu bug'larÄ± buldukÃ§a ve yeni Ã¶zellikler ekledikÃ§e code'u iyileÅŸtiririz (muhtemelen yeni bug'lar da ekleyerek ğŸ˜…).

### KÃ¼Ã§Ã¼k Hatalar Otomatik YÃ¶netilir { #small-errors-automatically-handled }

FastAPI ile web API geliÅŸtirirken, code'umuzda bir hata olursa FastAPI genellikle bunu hatayÄ± tetikleyen tek request ile sÄ±nÄ±rlar. ğŸ›¡

Client o request iÃ§in **500 Internal Server Error** alÄ±r; ancak uygulama tamamen Ã§Ã¶kÃ¼p durmak yerine sonraki request'ler iÃ§in Ã§alÄ±ÅŸmaya devam eder.

### Daha BÃ¼yÃ¼k Hatalar - Ã‡Ã¶kmeler { #bigger-errors-crashes }

Yine de bazÄ± durumlarda, yazdÄ±ÄŸÄ±mÄ±z bir code **tÃ¼m uygulamayÄ± Ã§Ã¶kertip** Uvicorn ve Python'Ä±n crash olmasÄ±na neden olabilir. ğŸ’¥

BÃ¶yle bir durumda, tek bir noktadaki hata yÃ¼zÃ¼nden uygulamanÄ±n Ã¶lÃ¼ kalmasÄ±nÄ± istemezsiniz; bozuk olmayan *path operations* en azÄ±ndan Ã§alÄ±ÅŸmaya devam etsin istersiniz.

### Crash SonrasÄ± Yeniden BaÅŸlatma { #restart-after-crash }

Ancak Ã§alÄ±ÅŸan **process**'i Ã§Ã¶kerten gerÃ§ekten kÃ¶tÃ¼ hatalarda, process'i **yeniden baÅŸlatmaktan** sorumlu harici bir bileÅŸen istersiniz; en azÄ±ndan birkaÃ§ kez...

/// tip | Ä°pucu

...Yine de uygulama **hemen crash oluyorsa**, onu sonsuza kadar yeniden baÅŸlatmaya Ã§alÄ±ÅŸmanÄ±n pek anlamÄ± yoktur. BÃ¶yle durumlarÄ± bÃ¼yÃ¼k ihtimalle geliÅŸtirme sÄ±rasÄ±nda ya da en geÃ§ deploy'dan hemen sonra fark edersiniz.

O yÃ¼zden ana senaryoya odaklanalÄ±m: Gelecekte bazÄ± Ã¶zel durumlarda tamamen Ã§Ã¶kebilir ve yine de yeniden baÅŸlatmak mantÄ±klÄ±dÄ±r.

///

UygulamanÄ±zÄ± yeniden baÅŸlatmakla gÃ¶revli bileÅŸenin **harici bir bileÅŸen** olmasÄ±nÄ± istersiniz. Ã‡Ã¼nkÃ¼ o noktada Uvicorn ve Python ile birlikte aynÄ± uygulama zaten crash olmuÅŸtur; aynÄ± app'in iÃ§indeki aynÄ± code'un bunu dÃ¼zeltmek iÃ§in yapabileceÄŸi bir ÅŸey kalmaz.

### Otomatik Yeniden BaÅŸlatma iÃ§in Ã–rnek AraÃ§lar { #example-tools-to-restart-automatically }

Ã‡oÄŸu durumda, **startup'ta programÄ± Ã§alÄ±ÅŸtÄ±rmak** iÃ§in kullanÄ±lan aracÄ±n aynÄ±sÄ± otomatik **restart**'larÄ± yÃ¶netmek iÃ§in de kullanÄ±lÄ±r.

Ã–rneÄŸin bu ÅŸunlarla yÃ¶netilebilir:

* Docker
* Kubernetes
* Docker Compose
* Docker in Swarm Mode
* Systemd
* Supervisor
* Bir cloud provider tarafÄ±ndan servislerinin parÃ§asÄ± olarak iÃ§eride yÃ¶netilmesi
* DiÄŸerleri...

## Replikasyon - Process'ler ve Bellek { #replication-processes-and-memory }

FastAPI uygulamasÄ±nda, Uvicorn'u Ã§alÄ±ÅŸtÄ±ran `fastapi` komutu gibi bir server programÄ± kullanÄ±rken, uygulamayÄ± **tek bir process** iÃ§inde bir kez Ã§alÄ±ÅŸtÄ±rmak bile aynÄ± anda birden fazla client'a hizmet verebilir.

Ancak birÃ§ok durumda, aynÄ± anda birden fazla worker process Ã§alÄ±ÅŸtÄ±rmak istersiniz.

### Birden Fazla Process - Worker'lar { #multiple-processes-workers }

Tek bir process'in karÅŸÄ±layabileceÄŸinden daha fazla client'Ä±nÄ±z varsa (Ã¶rneÄŸin sanal makine Ã§ok bÃ¼yÃ¼k deÄŸilse) ve server CPU'sunda **birden fazla core** varsa, aynÄ± uygulamayla **birden fazla process** Ã§alÄ±ÅŸtÄ±rÄ±p tÃ¼m request'leri bunlara daÄŸÄ±tabilirsiniz.

AynÄ± API programÄ±nÄ±n **birden fazla process**'ini Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, bunlara genellikle **worker** denir.

### Worker Process'ler ve Port'lar { #worker-processes-and-ports }

[HTTPS hakkÄ±ndaki dokÃ¼manda](https.md){.internal-link target=_blank} bir server'da aynÄ± port ve IP adresi kombinasyonunu yalnÄ±zca tek bir process'in dinleyebileceÄŸini hatÄ±rlÄ±yor musunuz?

Bu hÃ¢lÃ¢ geÃ§erli.

DolayÄ±sÄ±yla **aynÄ± anda birden fazla process** Ã§alÄ±ÅŸtÄ±rabilmek iÃ§in, **port** Ã¼zerinde dinleyen **tek bir process** olmalÄ± ve bu process iletiÅŸimi bir ÅŸekilde worker process'lere aktarmalÄ±dÄ±r.

### Process BaÅŸÄ±na Bellek { #memory-per-process }

Program belleÄŸe bir ÅŸeyler yÃ¼klediÄŸindeâ€”Ã¶rneÄŸin bir deÄŸiÅŸkende bir machine learning modelini veya bÃ¼yÃ¼k bir dosyanÄ±n iÃ§eriÄŸini tutmak gibiâ€”bunlarÄ±n hepsi server'Ä±n **belleÄŸini (RAM)** tÃ¼ketir.

Ve birden fazla process normalde **belleÄŸi paylaÅŸmaz**. Yani her Ã§alÄ±ÅŸan process'in kendi verileri, deÄŸiÅŸkenleri ve belleÄŸi vardÄ±r. Code'unuz Ã§ok bellek tÃ¼ketiyorsa, **her process** buna denk bir miktar bellek tÃ¼ketir.

### Server BelleÄŸi { #server-memory }

Ã–rneÄŸin code'unuz **1 GB** boyutunda bir Machine Learning modelini yÃ¼klÃ¼yorsa, API'niz tek process ile Ã§alÄ±ÅŸÄ±rken en az 1 GB RAM tÃ¼ketir. **4 process** (4 worker) baÅŸlatÄ±rsanÄ±z her biri 1 GB RAM tÃ¼ketir. Yani toplamda API'niz **4 GB RAM** tÃ¼ketir.

Uzak server'Ä±nÄ±z veya sanal makineniz yalnÄ±zca 3 GB RAM'e sahipse, 4 GB'tan fazla RAM yÃ¼klemeye Ã§alÄ±ÅŸmak sorun Ã§Ä±karÄ±r. ğŸš¨

### Birden Fazla Process - Bir Ã–rnek { #multiple-processes-an-example }

Bu Ã¶rnekte, iki adet **Worker Process** baÅŸlatÄ±p kontrol eden bir **Manager Process** vardÄ±r.

Bu Manager Process bÃ¼yÃ¼k ihtimalle IP Ã¼zerindeki **port**'u dinleyen sÃ¼reÃ§tir ve tÃ¼m iletiÅŸimi worker process'lere aktarÄ±r.

Worker process'ler uygulamanÄ±zÄ± Ã§alÄ±ÅŸtÄ±ran process'lerdir; bir **request** alÄ±p bir **response** dÃ¶ndÃ¼rmek iÃ§in asÄ±l hesaplamalarÄ± yaparlar ve sizin RAM'de deÄŸiÅŸkenlere koyduÄŸunuz her ÅŸeyi yÃ¼klerler.

<img src="/img/deployment/concepts/process-ram.drawio.svg">

Elbette aynÄ± makinede, uygulamanÄ±z dÄ±ÅŸÄ±nda da muhtemelen **baÅŸka process**'ler Ã§alÄ±ÅŸÄ±r.

Ä°lginÃ§ bir detay: Her process'in kullandÄ±ÄŸÄ± **CPU** yÃ¼zdesi zaman iÃ§inde Ã§ok **deÄŸiÅŸken** olabilir; ancak **bellek (RAM)** genellikle az Ã§ok **stabil** kalÄ±r.

EÄŸer API'niz her seferinde benzer miktarda hesaplama yapÄ±yorsa ve Ã§ok sayÄ±da client'Ä±nÄ±z varsa, **CPU kullanÄ±mÄ±** da muhtemelen *stabil olur* (hÄ±zlÄ± hÄ±zlÄ± sÃ¼rekli yÃ¼kselip alÃ§almak yerine).

### Replikasyon AraÃ§larÄ± ve Stratejileri Ã–rnekleri { #examples-of-replication-tools-and-strategies }

Bunu baÅŸarmak iÃ§in farklÄ± yaklaÅŸÄ±mlar olabilir. Sonraki bÃ¶lÃ¼mlerde, Ã¶rneÄŸin Docker ve container'lar konuÅŸurken, belirli stratejileri daha detaylÄ± anlatacaÄŸÄ±m.

Dikkate almanÄ±z gereken ana kÄ±sÄ±t ÅŸudur: **public IP** Ã¼zerindeki **port**'u yÃ¶neten **tek** bir bileÅŸen olmalÄ±. SonrasÄ±nda bu bileÅŸenin, replikasyonla Ã§oÄŸaltÄ±lmÄ±ÅŸ **process/worker**'lara iletiÅŸimi **aktarmanÄ±n** bir yoluna sahip olmasÄ± gerekir.

OlasÄ± kombinasyonlar ve stratejiler:

* `--workers` ile **Uvicorn**
    * Bir Uvicorn **process manager** **IP** ve **port** Ã¼zerinde dinler ve **birden fazla Uvicorn worker process** baÅŸlatÄ±r.
* **Kubernetes** ve diÄŸer daÄŸÄ±tÄ±k **container sistemleri**
    * **Kubernetes** katmanÄ±nda bir ÅŸey **IP** ve **port** Ã¼zerinde dinler. Replikasyon, her birinde **tek bir Uvicorn process** Ã§alÄ±ÅŸan **birden fazla container** ile yapÄ±lÄ±r.
* Bunu sizin yerinize yapan **cloud service**'ler
    * Cloud service muhtemelen **replikasyonu sizin yerinize yÃ¶netir**. Size Ã§alÄ±ÅŸtÄ±rÄ±lacak **bir process** veya kullanÄ±lacak bir **container image** tanÄ±mlama imkÃ¢nÄ± verebilir; her durumda bÃ¼yÃ¼k ihtimalle **tek bir Uvicorn process** olur ve bunu Ã§oÄŸaltmaktan cloud service sorumlu olur.

/// tip | Ä°pucu

**Container**, Docker veya Kubernetes ile ilgili bazÄ± maddeler ÅŸimdilik Ã§ok anlamlÄ± gelmiyorsa dert etmeyin.

Container image'larÄ±, Docker, Kubernetes vb. konularÄ± ilerideki bir bÃ¶lÃ¼mde daha detaylÄ± anlatacaÄŸÄ±m: [Container'larda FastAPI - Docker](docker.md){.internal-link target=_blank}.

///

## BaÅŸlatmadan Ã–nceki AdÄ±mlar { #previous-steps-before-starting }

UygulamanÄ±zÄ± **baÅŸlatmadan Ã¶nce** bazÄ± adÄ±mlar yapmak isteyeceÄŸiniz birÃ§ok durum vardÄ±r.

Ã–rneÄŸin **database migrations** Ã§alÄ±ÅŸtÄ±rmak isteyebilirsiniz.

Ancak Ã§oÄŸu durumda, bu adÄ±mlarÄ± yalnÄ±zca **bir kez** Ã§alÄ±ÅŸtÄ±rmak istersiniz.

Bu yÃ¼zden, uygulamayÄ± baÅŸlatmadan Ã¶nce bu **Ã¶n adÄ±mlarÄ±** Ã§alÄ±ÅŸtÄ±racak **tek bir process** olmasÄ±nÄ± istersiniz.

Ve daha sonra uygulamanÄ±n kendisi iÃ§in **birden fazla process** (birden fazla worker) baÅŸlatsanÄ±z bile, bu Ã¶n adÄ±mlarÄ± Ã§alÄ±ÅŸtÄ±ranÄ±n *yine* tek process olduÄŸundan emin olmalÄ±sÄ±nÄ±z. Bu adÄ±mlar **birden fazla process** tarafÄ±ndan Ã§alÄ±ÅŸtÄ±rÄ±lsaydÄ±, iÅŸi **paralel** ÅŸekilde tekrarlarlardÄ±. AdÄ±mlar database migration gibi hassas bir ÅŸeyse, birbirleriyle Ã§akÄ±ÅŸÄ±p Ã§atÄ±ÅŸma Ã§Ä±karabilirler.

Elbette bazÄ± durumlarda Ã¶n adÄ±mlarÄ± birden fazla kez Ã§alÄ±ÅŸtÄ±rmak sorun deÄŸildir; bu durumda yÃ¶netmesi Ã§ok daha kolay olur.

/// tip | Ä°pucu

AyrÄ±ca, kurulumunuza baÄŸlÄ± olarak bazÄ± durumlarda uygulamanÄ±zÄ± baÅŸlatmadan Ã¶nce **hiÃ§ Ã¶n adÄ±ma ihtiyaÃ§ duymayabilirsiniz**.

Bu durumda bunlarÄ±n hiÃ§birini dÃ¼ÅŸÃ¼nmeniz gerekmez. ğŸ¤·

///

### Ã–n AdÄ±mlar iÃ§in Strateji Ã–rnekleri { #examples-of-previous-steps-strategies }

Bu konu, **sisteminizi nasÄ±l deploy ettiÄŸinize** Ã§ok baÄŸlÄ±dÄ±r ve muhtemelen programlarÄ± nasÄ±l baÅŸlattÄ±ÄŸÄ±nÄ±z, restart'larÄ± nasÄ±l yÃ¶nettiÄŸiniz vb. ile baÄŸlantÄ±lÄ±dÄ±r.

BazÄ± olasÄ± fikirler:

* Kubernetes'te, app container'Ä±nÄ±zdan Ã¶nce Ã§alÄ±ÅŸan bir "Init Container"
* Ã–n adÄ±mlarÄ± Ã§alÄ±ÅŸtÄ±rÄ±p sonra uygulamanÄ±zÄ± baÅŸlatan bir bash script
    * Yine de o bash script'i baÅŸlatmak/restart etmek, hatalarÄ± tespit etmek vb. iÃ§in bir mekanizmaya ihtiyacÄ±nÄ±z olur.

/// tip | Ä°pucu

Bunu container'larla nasÄ±l yapabileceÄŸinize dair daha somut Ã¶rnekleri ilerideki bir bÃ¶lÃ¼mde anlatacaÄŸÄ±m: [Container'larda FastAPI - Docker](docker.md){.internal-link target=_blank}.

///

## Kaynak KullanÄ±mÄ± { #resource-utilization }

Server(lar)Ä±nÄ±z bir **kaynaktÄ±r**. ProgramlarÄ±nÄ±zla CPU'lardaki hesaplama zamanÄ±nÄ± ve mevcut RAM belleÄŸini tÃ¼ketebilir veya **kullanabilirsiniz**.

Sistem kaynaklarÄ±nÄ±n ne kadarÄ±nÄ± tÃ¼ketmek/kullanmak istersiniz? "Az" demek kolaydÄ±r; ancak pratikte hedef genellikle **Ã§Ã¶kmeden mÃ¼mkÃ¼n olduÄŸunca fazla** kullanmaktÄ±r.

3 server iÃ§in para Ã¶dÃ¼yor ama onlarÄ±n RAM ve CPU'sunun yalnÄ±zca kÃ¼Ã§Ã¼k bir kÄ±smÄ±nÄ± kullanÄ±yorsanÄ±z, muhtemelen **para israf ediyorsunuz** ğŸ’¸ ve muhtemelen **elektrik tÃ¼ketimini** de gereksiz yere artÄ±rÄ±yorsunuz ğŸŒ vb.

Bu durumda 2 server ile devam edip onlarÄ±n kaynaklarÄ±nÄ± (CPU, bellek, disk, aÄŸ bant geniÅŸliÄŸi vb.) daha yÃ¼ksek oranlarda kullanmak daha iyi olabilir.

Ã–te yandan, 2 server'Ä±nÄ±z var ve CPU ile RAM'in **%100**'Ã¼nÃ¼ kullanÄ±yorsanÄ±z, bir noktada bir process daha fazla bellek ister; server diski "bellek" gibi kullanmak zorunda kalÄ±r (binlerce kat daha yavaÅŸ olabilir) ya da hatta **crash** edebilir. Ya da bir process bir hesaplama yapmak ister ve CPU tekrar boÅŸalana kadar beklemek zorunda kalÄ±r.

Bu senaryoda **bir server daha** eklemek ve bazÄ± process'leri orada Ã§alÄ±ÅŸtÄ±rmak daha iyi olur; bÃ¶ylece hepsinin **yeterli RAM'i ve CPU zamanÄ±** olur.

AyrÄ±ca, herhangi bir sebeple API'nizde bir kullanÄ±m **spike**'Ä± olma ihtimali de vardÄ±r. Belki viral olur, belki baÅŸka servisler veya bot'lar kullanmaya baÅŸlar. Bu durumlarda gÃ¼vende olmak iÃ§in ekstra kaynak isteyebilirsiniz.

Hedef olarak **keyfi bir sayÄ±** belirleyebilirsiniz; Ã¶rneÄŸin kaynak kullanÄ±mÄ±nÄ± **%50 ile %90 arasÄ±nda** tutmak gibi. Ã–nemli olan, bunlarÄ±n muhtemelen Ã¶lÃ§mek isteyeceÄŸiniz ve deployment'larÄ±nÄ±zÄ± ayarlamak iÃ§in kullanacaÄŸÄ±nÄ±z ana metrikler olmasÄ±dÄ±r.

Server'Ä±nÄ±zda CPU ve RAM kullanÄ±mÄ±nÄ± veya her process'in ne kadar kullandÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in `htop` gibi basit araÃ§larÄ± kullanabilirsiniz. Ya da server'lar arasÄ±nda daÄŸÄ±tÄ±k Ã§alÄ±ÅŸan daha karmaÅŸÄ±k monitoring araÃ§larÄ± kullanabilirsiniz.

## Ã–zet { #recap }

UygulamanÄ±zÄ± nasÄ±l deploy edeceÄŸinize karar verirken aklÄ±nÄ±zda tutmanÄ±z gereken ana kavramlarÄ±n bazÄ±larÄ±nÄ± okudunuz:

* GÃ¼venlik - HTTPS
* Startup'ta Ã§alÄ±ÅŸtÄ±rma
* Yeniden baÅŸlatmalar
* Replikasyon (Ã§alÄ±ÅŸan process sayÄ±sÄ±)
* Bellek
* BaÅŸlatmadan Ã¶nceki adÄ±mlar

Bu fikirleri ve nasÄ±l uygulayacaÄŸÄ±nÄ±zÄ± anlamak, deployment'larÄ±nÄ±zÄ± yapÄ±landÄ±rÄ±rken ve ince ayar yaparken ihtiyaÃ§ duyacaÄŸÄ±nÄ±z sezgiyi kazanmanÄ±zÄ± saÄŸlamalÄ±dÄ±r. ğŸ¤“

Sonraki bÃ¶lÃ¼mlerde, izleyebileceÄŸiniz stratejilere dair daha somut Ã¶rnekler paylaÅŸacaÄŸÄ±m. ğŸš€
