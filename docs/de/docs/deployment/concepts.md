# Deployment-Konzepte

Bei dem Deployment â€“ der Bereitstellung â€“ einer **FastAPI**-Anwendung, oder eigentlich jeder Art von Web-API, gibt es mehrere Konzepte, die Sie wahrscheinlich interessieren, und mithilfe der Sie die **am besten geeignete** Methode zur **Bereitstellung Ihrer Anwendung** finden kÃ¶nnen.

Einige wichtige Konzepte sind:

* Sicherheit â€“ HTTPS
* Beim Hochfahren ausfÃ¼hren
* Neustarts
* Replikation (die Anzahl der laufenden Prozesse)
* Arbeitsspeicher
* Schritte vor dem Start

Wir werden sehen, wie diese sich auf das **Deployment** auswirken.

Letztendlich besteht das ultimative Ziel darin, **Ihre API-Clients** auf **sichere** Weise zu bedienen, um **Unterbrechungen** zu vermeiden und die **Rechenressourcen** (z. B. entfernte Server/virtuelle Maschinen) so effizient wie mÃ¶glich zu nutzen. ğŸš€

Ich erzÃ¤hle Ihnen hier etwas mehr Ã¼ber diese **Konzepte**, was Ihnen hoffentlich die **Intuition** gibt, die Sie benÃ¶tigen, um zu entscheiden, wie Sie Ihre API in sehr unterschiedlichen Umgebungen bereitstellen, mÃ¶glicherweise sogar in **zukÃ¼nftigen**, die jetzt noch nicht existieren.

Durch die BerÃ¼cksichtigung dieser Konzepte kÃ¶nnen Sie die beste Variante der Bereitstellung **Ihrer eigenen APIs** **evaluieren und konzipieren**.

In den nÃ¤chsten Kapiteln werde ich Ihnen mehr **konkrete Rezepte** fÃ¼r die Bereitstellung von FastAPI-Anwendungen geben.

Aber schauen wir uns zunÃ¤chst einmal diese grundlegenden **konzeptionellen Ideen** an. Diese Konzepte gelten auch fÃ¼r jede andere Art von Web-API. ğŸ’¡

## Sicherheit â€“ HTTPS

Im [vorherigen Kapitel Ã¼ber HTTPS](https.md){.internal-link target=_blank} haben wir erfahren, wie HTTPS VerschlÃ¼sselung fÃ¼r Ihre API bereitstellt.

Wir haben auch gesehen, dass HTTPS normalerweise von einer Komponente **auÃŸerhalb** Ihres Anwendungsservers bereitgestellt wird, einem **TLS-Terminierungsproxy**.

Und es muss etwas geben, das fÃ¼r die **Erneuerung der HTTPS-Zertifikate** zustÃ¤ndig ist, es kÃ¶nnte sich um dieselbe Komponente handeln oder um etwas anderes.

### Beispieltools fÃ¼r HTTPS

Einige der Tools, die Sie als TLS-Terminierungsproxy verwenden kÃ¶nnen, sind:

* Traefik
    * Handhabt automatisch Zertifikat-Erneuerungen âœ¨
* Caddy
    * Handhabt automatisch Zertifikat-Erneuerungen âœ¨
* Nginx
    * Mit einer externen Komponente wie Certbot fÃ¼r Zertifikat-Erneuerungen
* HAProxy
    * Mit einer externen Komponente wie Certbot fÃ¼r Zertifikat-Erneuerungen
* Kubernetes mit einem Ingress Controller wie Nginx
    * Mit einer externen Komponente wie cert-manager fÃ¼r Zertifikat-Erneuerungen
* Es wird intern von einem Cloud-Anbieter als Teil seiner Dienste verwaltet (siehe unten ğŸ‘‡)

Eine andere MÃ¶glichkeit besteht darin, dass Sie einen **Cloud-Dienst** verwenden, der den grÃ¶ÃŸten Teil der Arbeit Ã¼bernimmt, einschlieÃŸlich der Einrichtung von HTTPS. Er kÃ¶nnte einige EinschrÃ¤nkungen haben oder Ihnen mehr in Rechnung stellen, usw. In diesem Fall mÃ¼ssten Sie jedoch nicht selbst einen TLS-Terminierungsproxy einrichten.

In den nÃ¤chsten Kapiteln zeige ich Ihnen einige konkrete Beispiele.

---

Die nÃ¤chsten zu berÃ¼cksichtigenden Konzepte drehen sich dann um das Programm, das Ihre eigentliche API ausfÃ¼hrt (z. B. Uvicorn).

## Programm und Prozess

Wir werden viel Ã¼ber den laufenden â€**Prozess**â€œ sprechen, daher ist es nÃ¼tzlich, Klarheit darÃ¼ber zu haben, was das bedeutet und was der Unterschied zum Wort â€**Programm**â€œ ist.

### Was ist ein Programm?

Das Wort **Programm** wird hÃ¤ufig zur Beschreibung vieler Dinge verwendet:

* Der **Code**, den Sie schreiben, die **Python-Dateien**.
* Die **Datei**, die vom Betriebssystem **ausgefÃ¼hrt** werden kann, zum Beispiel: `python`, `python.exe` oder `uvicorn`.
* Ein bestimmtes Programm, wÃ¤hrend es auf dem Betriebssystem **lÃ¤uft**, die CPU nutzt und Dinge im Arbeitsspeicher ablegt. Dies wird auch als **Prozess** bezeichnet.

### Was ist ein Prozess?

Das Wort **Prozess** wird normalerweise spezifischer verwendet und bezieht sich nur auf das, was im Betriebssystem ausgefÃ¼hrt wird (wie im letzten Punkt oben):

* Ein bestimmtes Programm, wÃ¤hrend es auf dem Betriebssystem **ausgefÃ¼hrt** wird.
    * Dies bezieht sich weder auf die Datei noch auf den Code, sondern **speziell** auf das, was vom Betriebssystem **ausgefÃ¼hrt** und verwaltet wird.
* Jedes Programm, jeder Code **kann nur dann Dinge tun**, wenn er **ausgefÃ¼hrt** wird, wenn also ein **Prozess lÃ¤uft**.
* Der Prozess kann von Ihnen oder vom Betriebssystem **terminiert** (â€beendetâ€œ, â€gekilltâ€œ) werden. An diesem Punkt hÃ¶rt es auf zu laufen/ausgefÃ¼hrt zu werden und kann **keine Dinge mehr tun**.
* Hinter jeder Anwendung, die Sie auf Ihrem Computer ausfÃ¼hren, steckt ein Prozess, jedes laufende Programm, jedes Fenster usw. Und normalerweise laufen viele Prozesse **gleichzeitig**, wÃ¤hrend ein Computer eingeschaltet ist.
* Es kÃ¶nnen **mehrere Prozesse** desselben **Programms** gleichzeitig ausgefÃ¼hrt werden.

Wenn Sie sich den â€Task-Managerâ€œ oder â€Systemmonitorâ€œ (oder Ã¤hnliche Tools) in Ihrem Betriebssystem ansehen, kÃ¶nnen Sie viele dieser laufenden Prozesse sehen.

Und Sie werden beispielsweise wahrscheinlich feststellen, dass mehrere Prozesse dasselbe Browserprogramm ausfÃ¼hren (Firefox, Chrome, Edge, usw.). Normalerweise fÃ¼hren diese einen Prozess pro Browsertab sowie einige andere zusÃ¤tzliche Prozesse aus.

<img class="shadow" src="/img/deployment/concepts/image01.png">

---

Nachdem wir nun den Unterschied zwischen den Begriffen **Prozess** und **Programm** kennen, sprechen wir weiter Ã¼ber das Deployment.

## Beim Hochfahren ausfÃ¼hren

Wenn Sie eine Web-API erstellen, mÃ¶chten Sie in den meisten FÃ¤llen, dass diese **immer lÃ¤uft**, ununterbrochen, damit Ihre Clients immer darauf zugreifen kÃ¶nnen. Es sei denn natÃ¼rlich, Sie haben einen bestimmten Grund, warum Sie mÃ¶chten, dass diese nur in bestimmten Situationen ausgefÃ¼hrt wird. Meistens mÃ¶chten Sie jedoch, dass sie stÃ¤ndig ausgefÃ¼hrt wird und **verfÃ¼gbar** ist.

### Auf einem entfernten Server

Wenn Sie einen entfernten Server (einen Cloud-Server, eine virtuelle Maschine, usw.) einrichten, kÃ¶nnen Sie am einfachsten Uvicorn (oder Ã¤hnliches) manuell ausfÃ¼hren, genau wie bei der lokalen Entwicklung.

Und es wird funktionieren und **wÃ¤hrend der Entwicklung** nÃ¼tzlich sein.

Wenn Ihre Verbindung zum Server jedoch unterbrochen wird, wird der **laufende Prozess** wahrscheinlich abstÃ¼rzen.

Und wenn der Server neu gestartet wird (z. B. nach Updates oder Migrationen vom Cloud-Anbieter), werden Sie das wahrscheinlich **nicht bemerken**. Und deshalb wissen Sie nicht einmal, dass Sie den Prozess manuell neu starten mÃ¼ssen. Ihre API bleibt also einfach tot. ğŸ˜±

### Beim Hochfahren automatisch ausfÃ¼hren

Im Allgemeinen mÃ¶chten Sie wahrscheinlich, dass das Serverprogramm (z. B. Uvicorn) beim Hochfahren des Servers automatisch gestartet wird und kein **menschliches Eingreifen** erforderlich ist, sodass immer ein Prozess mit Ihrer API ausgefÃ¼hrt wird (z. B. Uvicorn, welches Ihre FastAPI-Anwendung ausfÃ¼hrt).

### Separates Programm

Um dies zu erreichen, haben Sie normalerweise ein **separates Programm**, welches sicherstellt, dass Ihre Anwendung beim Hochfahren ausgefÃ¼hrt wird. Und in vielen FÃ¤llen wÃ¼rde es auch sicherstellen, dass auch andere Komponenten oder Anwendungen ausgefÃ¼hrt werden, beispielsweise eine Datenbank.

### Beispieltools zur AusfÃ¼hrung beim Hochfahren

Einige Beispiele fÃ¼r Tools, die diese Aufgabe Ã¼bernehmen kÃ¶nnen, sind:

* Docker
* Kubernetes
* Docker Compose
* Docker im Schwarm-Modus
* Systemd
* Supervisor
* Es wird intern von einem Cloud-Anbieter im Rahmen seiner Dienste verwaltet
* Andere ...

In den nÃ¤chsten Kapiteln werde ich Ihnen konkretere Beispiele geben.

## Neustart

Ã„hnlich wie Sie sicherstellen mÃ¶chten, dass Ihre Anwendung beim Hochfahren ausgefÃ¼hrt wird, mÃ¶chten Sie wahrscheinlich auch sicherstellen, dass diese nach Fehlern **neu gestartet** wird.

### Wir machen Fehler

Wir, als Menschen, machen stÃ¤ndig **Fehler**. Software hat fast *immer* **Bugs**, die an verschiedenen Stellen versteckt sind. ğŸ›

Und wir als Entwickler verbessern den Code stÃ¤ndig, wenn wir diese Bugs finden und neue Funktionen implementieren (und mÃ¶glicherweise auch neue Bugs hinzufÃ¼gen ğŸ˜…).

### Kleine Fehler automatisch handhaben

Wenn beim Erstellen von Web-APIs mit FastAPI ein Fehler in unserem Code auftritt, wird FastAPI ihn normalerweise dem einzelnen Request zurÃ¼ckgeben, der den Fehler ausgelÃ¶st hat. ğŸ›¡

Der Client erhÃ¤lt fÃ¼r diesen Request einen **500 Internal Server Error**, aber die Anwendung arbeitet bei den nÃ¤chsten Requests weiter, anstatt einfach komplett abzustÃ¼rzen.

### GrÃ¶ÃŸere Fehler â€“ AbstÃ¼rze

Dennoch kann es vorkommen, dass wir Code schreiben, der **die gesamte Anwendung zum Absturz bringt** und so zum Absturz von Uvicorn und Python fÃ¼hrt. ğŸ’¥

Und dennoch mÃ¶chten Sie wahrscheinlich nicht, dass die Anwendung tot bleibt, weil an einer Stelle ein Fehler aufgetreten ist. Sie mÃ¶chten wahrscheinlich, dass sie zumindest fÃ¼r die *Pfadoperationen*, die nicht fehlerhaft sind, **weiterlÃ¤uft**.

### Neustart nach Absturz

Aber in den FÃ¤llen mit wirklich schwerwiegenden Fehlern, die den laufenden **Prozess** zum Absturz bringen, benÃ¶tigen Sie eine externe Komponente, die den Prozess **neu startet**, zumindest ein paar Mal ...

/// tip | "Tipp"

... Obwohl es wahrscheinlich keinen Sinn macht, sie immer wieder neu zu starten, wenn die gesamte Anwendung einfach **sofort abstÃ¼rzt**. Aber in diesen FÃ¤llen werden Sie es wahrscheinlich wÃ¤hrend der Entwicklung oder zumindest direkt nach dem Deployment bemerken.

Konzentrieren wir uns also auf die HauptfÃ¤lle, in denen die Anwendung in bestimmten FÃ¤llen **in der Zukunft** vÃ¶llig abstÃ¼rzen kÃ¶nnte und es dann dennoch sinnvoll ist, sie neu zu starten.

///

Sie mÃ¶chten wahrscheinlich, dass eine **externe Komponente** fÃ¼r den Neustart Ihrer Anwendung verantwortlich ist, da zu diesem Zeitpunkt dieselbe Anwendung mit Uvicorn und Python bereits abgestÃ¼rzt ist und es daher nichts im selben Code derselben Anwendung gibt, was etwas dagegen tun kann.

### Beispieltools zum automatischen Neustart

In den meisten FÃ¤llen wird dasselbe Tool, das zum **AusfÃ¼hren des Programms beim Hochfahren** verwendet wird, auch fÃ¼r automatische **Neustarts** verwendet.

Dies kÃ¶nnte zum Beispiel erledigt werden durch:

* Docker
* Kubernetes
* Docker Compose
* Docker im Schwarm-Modus
* Systemd
* Supervisor
* Intern von einem Cloud-Anbieter im Rahmen seiner Dienste
* Andere ...

## Replikation â€“ Prozesse und Arbeitsspeicher

Wenn Sie eine FastAPI-Anwendung verwenden und ein Serverprogramm wie Uvicorn verwenden, kann **ein einzelner Prozess** mehrere Clients gleichzeitig bedienen.

In vielen FÃ¤llen mÃ¶chten Sie jedoch mehrere Prozesse gleichzeitig ausfÃ¼hren.

### Mehrere Prozesse â€“ Worker

Wenn Sie mehr Clients haben, als ein einzelner Prozess verarbeiten kann (z. B. wenn die virtuelle Maschine nicht sehr groÃŸ ist) und die CPU des Servers **mehrere Kerne** hat, dann kÃ¶nnten **mehrere Prozesse** gleichzeitig mit derselben Anwendung laufen und alle Requests unter sich verteilen.

Wenn Sie mit **mehreren Prozessen** dasselbe API-Programm ausfÃ¼hren, werden diese Ã¼blicherweise als **<abbr title="Arbeiter">Worker</abbr>** bezeichnet.

### Workerprozesse und Ports

Erinnern Sie sich aus der Dokumentation [Ãœber HTTPS](https.md){.internal-link target=_blank}, dass nur ein Prozess auf einer Kombination aus Port und IP-Adresse auf einem Server lauschen kann?

Das ist immer noch wahr.

Um also **mehrere Prozesse** gleichzeitig zu haben, muss es einen **einzelnen Prozess geben, der einen Port Ã¼berwacht**, welcher dann die Kommunikation auf irgendeine Weise an jeden Workerprozess Ã¼bertrÃ¤gt.

### Arbeitsspeicher pro Prozess

Wenn das Programm nun Dinge in den Arbeitsspeicher lÃ¤dt, zum Beispiel ein Modell fÃ¼r maschinelles Lernen in einer Variablen oder den Inhalt einer groÃŸen Datei in einer Variablen, verbraucht das alles **einen Teil des Arbeitsspeichers (RAM â€“ Random Access Memory)** des Servers.

Und mehrere Prozesse teilen sich normalerweise keinen Speicher. Das bedeutet, dass jeder laufende Prozess seine eigenen Dinge, eigenen Variablen und eigenen Speicher hat. Und wenn Sie in Ihrem Code viel Speicher verbrauchen, verbraucht **jeder Prozess** die gleiche Menge Speicher.

### Serverspeicher

Wenn Ihr Code beispielsweise ein Machine-Learning-Modell mit **1 GB GrÃ¶ÃŸe** lÃ¤dt und Sie einen Prozess mit Ihrer API ausfÃ¼hren, verbraucht dieser mindestens 1 GB RAM. Und wenn Sie **4 Prozesse** (4 Worker) starten, verbraucht jeder 1 GB RAM. Insgesamt verbraucht Ihre API also **4 GB RAM**.

Und wenn Ihr entfernter Server oder Ihre virtuelle Maschine nur Ã¼ber 3 GB RAM verfÃ¼gt, fÃ¼hrt der Versuch, mehr als 4 GB RAM zu laden, zu Problemen. ğŸš¨

### Mehrere Prozesse â€“ Ein Beispiel

Im folgenden Beispiel gibt es einen **Manager-Prozess**, welcher zwei **Workerprozesse** startet und steuert.

Dieser Manager-Prozess wÃ¤re wahrscheinlich derjenige, welcher der IP am **Port** lauscht. Und er wÃ¼rde die gesamte Kommunikation an die Workerprozesse weiterleiten.

Diese Workerprozesse wÃ¼rden Ihre Anwendung ausfÃ¼hren, sie wÃ¼rden die Hauptberechnungen durchfÃ¼hren, um einen **Request** entgegenzunehmen und eine **Response** zurÃ¼ckzugeben, und sie wÃ¼rden alles, was Sie in Variablen einfÃ¼gen, in den RAM laden.

<img src="/img/deployment/concepts/process-ram.svg">

Und natÃ¼rlich wÃ¼rden auf derselben Maschine neben Ihrer Anwendung wahrscheinlich auch **andere Prozesse** laufen.

Ein interessantes Detail ist dabei, dass der Prozentsatz der von jedem Prozess verwendeten **CPU** im Laufe der Zeit stark **variieren** kann, der **Arbeitsspeicher (RAM)** jedoch normalerweise mehr oder weniger **stabil** bleibt.

Wenn Sie eine API haben, die jedes Mal eine vergleichbare Menge an Berechnungen durchfÃ¼hrt, und Sie viele Clients haben, dann wird die **CPU-Auslastung** wahrscheinlich *ebenfalls stabil sein* (anstatt stÃ¤ndig schnell zu steigen und zu fallen).

### Beispiele fÃ¼r Replikation-Tools und -Strategien

Es gibt mehrere AnsÃ¤tze, um dies zu erreichen, und ich werde Ihnen in den nÃ¤chsten Kapiteln mehr Ã¼ber bestimmte Strategien erzÃ¤hlen, beispielsweise wenn es um Docker und Container geht.

Die wichtigste zu berÃ¼cksichtigende EinschrÃ¤nkung besteht darin, dass es eine **einzelne** Komponente geben muss, welche die **Ã¶ffentliche IP** auf dem **Port** verwaltet. Und dann muss diese irgendwie die Kommunikation **weiterleiten**, an die replizierten **Prozesse/Worker**.

Hier sind einige mÃ¶gliche Kombinationen und Strategien:

* **Gunicorn**, welches **Uvicorn-Worker** managt
    * Gunicorn wÃ¤re der **Prozessmanager**, der die **IP** und den **Port** Ã¼berwacht, die Replikation wÃ¼rde durch **mehrere Uvicorn-Workerprozesse** erfolgen
* **Uvicorn**, welches **Uvicorn-Worker** managt
    * Ein Uvicorn-**Prozessmanager** wÃ¼rde der **IP** am **Port** lauschen, und er wÃ¼rde **mehrere Uvicorn-Workerprozesse** starten.
* **Kubernetes** und andere verteilte **Containersysteme**
    * Etwas in der **Kubernetes**-Ebene wÃ¼rde die **IP** und den **Port** abhÃ¶ren. Die Replikation hÃ¤tte **mehrere Container**, in jedem wird jeweils **ein Uvicorn-Prozess** ausgefÃ¼hrt.
* **Cloud-Dienste**, welche das fÃ¼r Sie erledigen
    * Der Cloud-Dienst wird wahrscheinlich **die Replikation fÃ¼r Sie Ã¼bernehmen**. Er wÃ¼rde Sie mÃ¶glicherweise **einen auszufÃ¼hrenden Prozess** oder ein **zu verwendendes Container-Image** definieren lassen, in jedem Fall wÃ¤re es hÃ¶chstwahrscheinlich **ein einzelner Uvicorn-Prozess**, und der Cloud-Dienst wÃ¤re auch verantwortlich fÃ¼r die Replikation.

/// tip | "Tipp"

Machen Sie sich keine Sorgen, wenn einige dieser Punkte zu **Containern**, Docker oder Kubernetes noch nicht viel Sinn ergeben.

Ich werde Ihnen in einem zukÃ¼nftigen Kapitel mehr Ã¼ber Container-Images, Docker, Kubernetes, usw. erzÃ¤hlen: [FastAPI in Containern â€“ Docker](docker.md){.internal-link target=_blank}.

///

## Schritte vor dem Start

Es gibt viele FÃ¤lle, in denen Sie, **bevor Sie Ihre Anwendung starten**, einige Schritte ausfÃ¼hren mÃ¶chten.

Beispielsweise mÃ¶chten Sie mÃ¶glicherweise **Datenbankmigrationen** ausfÃ¼hren.

In den meisten FÃ¤llen mÃ¶chten Sie diese Schritte jedoch nur **einmal** ausfÃ¼hren.

Sie mÃ¶chten also einen **einzelnen Prozess** haben, um diese **Vorab-Schritte** auszufÃ¼hren, bevor Sie die Anwendung starten.

Und Sie mÃ¼ssen sicherstellen, dass es sich um einen einzelnen Prozess handelt, der die Vorab-Schritte ausfÃ¼hrt, *auch* wenn Sie anschlieÃŸend **mehrere Prozesse** (mehrere Worker) fÃ¼r die Anwendung selbst starten. Wenn diese Schritte von **mehreren Prozessen** ausgefÃ¼hrt wÃ¼rden, wÃ¼rden diese die Arbeit **verdoppeln**, indem sie sie **parallel** ausfÃ¼hren, und wenn es sich bei den Schritten um etwas Delikates wie eine Datenbankmigration handelt, kÃ¶nnte das miteinander Konflikte verursachen.

NatÃ¼rlich gibt es FÃ¤lle, in denen es kein Problem darstellt, die Vorab-Schritte mehrmals auszufÃ¼hren. In diesem Fall ist die Handhabung viel einfacher.

/// tip | "Tipp"

Bedenken Sie auÃŸerdem, dass Sie, abhÃ¤ngig von Ihrer Einrichtung, in manchen FÃ¤llen **gar keine Vorab-Schritte** benÃ¶tigen, bevor Sie die Anwendung starten.

In diesem Fall mÃ¼ssen Sie sich darÃ¼ber keine Sorgen machen. ğŸ¤·

///

### Beispiele fÃ¼r Strategien fÃ¼r Vorab-Schritte

Es hÃ¤ngt **stark** davon ab, wie Sie **Ihr System bereitstellen**, und hÃ¤ngt wahrscheinlich mit der Art und Weise zusammen, wie Sie Programme starten, Neustarts durchfÃ¼hren, usw.

Hier sind einige mÃ¶gliche Ideen:

* Ein â€Init-Containerâ€œ in Kubernetes, der vor Ihrem Anwendungs-Container ausgefÃ¼hrt wird
* Ein Bash-Skript, das die Vorab-Schritte ausfÃ¼hrt und dann Ihre Anwendung startet
    * Sie benÃ¶tigen immer noch eine MÃ¶glichkeit, *dieses* Bash-Skript zu starten/neu zu starten, Fehler zu erkennen, usw.

/// tip | "Tipp"

Konkretere Beispiele hierfÃ¼r mit Containern gebe ich Ihnen in einem spÃ¤teren Kapitel: [FastAPI in Containern â€“ Docker](docker.md){.internal-link target=_blank}.

///

## Ressourcennutzung

Ihr(e) Server ist (sind) eine **Ressource**, welche Sie mit Ihren Programmen, der Rechenzeit auf den CPUs und dem verfÃ¼gbaren RAM-Speicher verbrauchen oder **nutzen** kÃ¶nnen.

Wie viele Systemressourcen mÃ¶chten Sie verbrauchen/nutzen? Sie mÃ¶gen â€nicht vielâ€œ denken, aber in Wirklichkeit mÃ¶chten Sie tatsÃ¤chlich **so viel wie mÃ¶glich ohne Absturz** verwenden.

Wenn Sie fÃ¼r drei Server bezahlen, aber nur wenig von deren RAM und CPU nutzen, **verschwenden Sie wahrscheinlich Geld** ğŸ’¸ und wahrscheinlich **Strom fÃ¼r den Server** ğŸŒ, usw.

In diesem Fall kÃ¶nnte es besser sein, nur zwei Server zu haben und einen hÃ¶heren Prozentsatz von deren Ressourcen zu nutzen (CPU, Arbeitsspeicher, Festplatte, Netzwerkbandbreite, usw.).

Wenn Sie andererseits Ã¼ber zwei Server verfÃ¼gen und **100 % ihrer CPU und ihres RAM** nutzen, wird irgendwann ein Prozess nach mehr Speicher fragen und der Server muss die Festplatte als â€Speicherâ€œ verwenden (was tausendmal langsamer sein kann) oder er kÃ¶nnte sogar **abstÃ¼rzen**. Oder ein Prozess muss mÃ¶glicherweise einige Berechnungen durchfÃ¼hren und mÃ¼sste warten, bis die CPU wieder frei ist.

In diesem Fall wÃ¤re es besser, **einen zusÃ¤tzlichen Server** zu besorgen und einige Prozesse darauf auszufÃ¼hren, damit alle Ã¼ber **genug RAM und CPU-Zeit** verfÃ¼gen.

Es besteht auch die MÃ¶glichkeit, dass es aus irgendeinem Grund zu **Spitzen** in der Nutzung Ihrer API kommt. Vielleicht ist diese viral gegangen, oder vielleicht haben andere Dienste oder Bots damit begonnen, sie zu nutzen. Und vielleicht mÃ¶chten Sie in solchen FÃ¤llen Ã¼ber zusÃ¤tzliche Ressourcen verfÃ¼gen, um auf der sicheren Seite zu sein.

Sie kÃ¶nnen eine **beliebige Zahl** festlegen, um beispielsweise eine Ressourcenauslastung zwischen **50Â % und 90Â %** anzustreben. Der Punkt ist, dass dies wahrscheinlich die wichtigen Dinge sind, die Sie messen und verwenden sollten, um Ihre Deployments zu optimieren.

Sie kÃ¶nnen einfache Tools wie `htop` verwenden, um die in Ihrem Server verwendete CPU und den RAM oder die von jedem Prozess verwendete Menge anzuzeigen. Oder Sie kÃ¶nnen komplexere Ãœberwachungstools verwenden, die mÃ¶glicherweise auf mehrere Server usw. verteilt sind.

## Zusammenfassung

Sie haben hier einige der wichtigsten Konzepte gelesen, die Sie wahrscheinlich berÃ¼cksichtigen mÃ¼ssen, wenn Sie entscheiden, wie Sie Ihre Anwendung bereitstellen:

* Sicherheit â€“ HTTPS
* Beim Hochfahren ausfÃ¼hren
* Neustarts
* Replikation (die Anzahl der laufenden Prozesse)
* Arbeitsspeicher
* Schritte vor dem Start

Das VerstÃ¤ndnis dieser Ideen und deren Anwendung sollte Ihnen die nÃ¶tige Intuition vermitteln, um bei der Konfiguration und Optimierung Ihrer Deployments Entscheidungen zu treffen. ğŸ¤“

In den nÃ¤chsten Abschnitten gebe ich Ihnen konkretere Beispiele fÃ¼r mÃ¶gliche Strategien, die Sie verfolgen kÃ¶nnen. ğŸš€
