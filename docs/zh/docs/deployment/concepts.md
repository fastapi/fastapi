# 部署概念 { #deployments-concepts }

在部署 **FastAPI** 应用，或者实际上任何类型的 Web API 时，有几个你可能会关心的概念，利用它们你可以找到**最合适的**方式来**部署你的应用**。

一些重要的概念是：

* 安全性 - HTTPS
* 启动时运行
* 重启
* 复制（运行的进程数量）
* 内存
* 启动前的前置步骤

我们将看看它们会如何影响**部署**。

最终的目标是能够以**安全**的方式**为你的 API 客户端**提供服务，**避免中断**，并尽可能高效地使用**计算资源**（例如远程服务器/虚拟机）。 🚀

我会在这里多讲一点这些**概念**，希望这能给你提供所需的**直觉**，从而决定如何在非常不同的环境中部署你的 API，甚至可能是在一些尚不存在的**未来**环境中。

通过考虑这些概念，你将能够**评估和设计**部署**你自己的 API**的最佳方式。

在接下来的章节中，我会给你更多用于部署 FastAPI 应用的**具体方案**。

但现在，我们先来看看这些重要的**概念性想法**。这些概念也适用于任何其他类型的 Web API。 💡

## 安全性 - HTTPS { #security-https }

在[上一章关于 HTTPS](https.md){.internal-link target=_blank} 中，我们了解了 HTTPS 如何为你的 API 提供加密。

我们还看到，HTTPS 通常由应用服务器**外部**的一个组件提供，即 **TLS 终止代理**。

并且必须有某个组件负责**续期 HTTPS 证书**，它可以是同一个组件，也可以是其他组件。

### HTTPS 示例工具 { #example-tools-for-https }

你可以用作 TLS 终止代理的一些工具包括：

* Traefik
    * 自动处理证书续期 ✨
* Caddy
    * 自动处理证书续期 ✨
* Nginx
    * 通过 Certbot 等外部组件进行证书续期
* HAProxy
    * 通过 Certbot 等外部组件进行证书续期
* 带有 Nginx 等 Ingress Controller 的 Kubernetes
    * 通过 cert-manager 等外部组件进行证书续期
* 由云服务商在其服务中内部处理（阅读下文 👇）

另一种选择是使用**云服务**来完成更多工作，包括设置 HTTPS。它可能有一些限制或向你收取更多费用等。但在这种情况下，你不必自己设置 TLS 终止代理。

我会在接下来的章节中向你展示一些具体示例。

---

接下来要考虑的概念都与运行实际 API 的程序有关（例如 Uvicorn）。

## 程序和进程 { #program-and-process }

我们会经常谈到正在运行的“**进程**”，所以弄清它的含义，以及它与“**程序**”这个词的区别会很有帮助。

### 什么是程序 { #what-is-a-program }

**程序**这个词通常用来描述很多东西：

* 你编写的 **代码**、**Python 文件**。
* 操作系统可以**执行**的**文件**，例如：`python`、`python.exe` 或 `uvicorn`。
* 某个程序在操作系统中**运行**时的实例，它会使用 CPU 并把内容存储到内存中。这也被称为**进程**。

### 什么是进程 { #what-is-a-process }

**进程**这个词通常更具体，仅指在操作系统中运行的东西（如上面的最后一点）：

* 某个程序在操作系统中**运行**时的实例。
    * 这不是指文件，也不是指代码，它**具体**指的是正在被操作系统**执行**并管理的东西。
* 任何程序、任何代码，只有在被**执行**时才**能做事**。也就是存在一个**正在运行的进程**时。
* 进程可以被你或操作系统**终止**（或“杀死”）。此时它会停止运行/执行，并且它将**不再能做事**。
* 你电脑上运行的每个应用背后都有一些进程，每个运行中的程序、每个窗口等都是如此。而且计算机开机时通常会**同时**运行许多进程。
* **同一程序**可以有**多个进程**同时运行。

如果你查看操作系统中的“任务管理器”或“系统监视器”（或类似工具），你会看到很多正在运行的进程。

例如，你可能会看到有多个进程在运行同一个浏览器程序（Firefox、Chrome、Edge 等）。它们通常每个标签页运行一个进程，再加上一些其他额外进程。

<img class="shadow" src="/img/deployment/concepts/image01.png">

---

现在我们知道了术语“进程”和“程序”之间的区别，让我们继续讨论部署。

## 启动时运行 { #running-on-startup }

在大多数情况下，当你创建 Web API 时，你希望它**始终运行**、不中断，以便你的客户端始终可以访问它。当然，除非你有特定原因希望它只在某些情况下运行，但大多数时候你希望它持续运行并且**可用**。

### 在远程服务器上 { #in-a-remote-server }

当你设置远程服务器（云服务器、虚拟机等）时，最简单的做法是像本地开发时一样，手动运行 `fastapi run`（它使用 Uvicorn）或类似的命令。

这在**开发过程中**会有效且有用。

但是，如果你与服务器的连接丢失，**正在运行的进程**很可能会终止。

并且如果服务器重启（例如更新后，或从云服务商迁移后），你可能**不会注意到它**。因此，你甚至不会知道需要手动重启进程。所以你的 API 会一直处于不可用状态。 😱

### 启动时自动运行 { #run-automatically-on-startup }

一般来说，你可能希望服务器程序（例如 Uvicorn）在服务器启动时自动启动，并且不需要任何**人为干预**，让始终有一个进程与你的 API 一起运行（例如由 Uvicorn 运行你的 FastAPI 应用）。

### 单独的程序 { #separate-program }

为实现这一点，你通常会有一个**单独的程序**来确保你的应用在启动时运行。在许多情况下，它还会确保其他组件或应用也运行，例如数据库。

### 启动时运行的示例工具 { #example-tools-to-run-at-startup }

可以完成这项工作的工具示例包括：

* Docker
* Kubernetes
* Docker Compose
* Docker in Swarm Mode
* Systemd
* Supervisor
* 由云服务商在其服务中内部处理
* 其他...

我会在接下来的章节中给你更具体的示例。

## 重启 { #restarts }

类似于确保应用在启动时运行，你可能还希望确保它在失败后会被**重启**。

### 我们会犯错误 { #we-make-mistakes }

我们人类总是会犯**错误**。软件几乎*总是*在不同的地方隐藏着 **bug**。 🐛

而我们作为开发者，会在发现这些 bug 并实现新功能时不断改进代码（也可能又引入新的 bug 😅）。

### 自动处理小错误 { #small-errors-automatically-handled }

使用 FastAPI 构建 Web API 时，如果代码中出现错误，FastAPI 通常会把错误限制在触发错误的那一次请求中。 🛡

客户端会在该请求收到 **500 Internal Server Error**，但应用会继续处理后续请求，而不是彻底崩溃。

### 更大的错误 - 崩溃 { #bigger-errors-crashes }

尽管如此，有时我们写的一些代码可能会**导致整个应用崩溃**，使 Uvicorn 和 Python 崩溃。 💥

即便如此，你可能也不希望应用因为某处的错误而一直不可用，你可能希望它至少能对未损坏的*路径操作* **继续运行**。

### 崩溃后重启 { #restart-after-crash }

但在那些严重错误导致正在运行的**进程**崩溃的情况下，你会希望有一个外部组件负责**重启**进程，至少重试几次...

/// tip | 提示

...不过如果整个应用只是**立即崩溃**，一直重启它可能没有意义。但在这些情况下，你可能会在开发过程中注意到它，或者至少在部署后立刻注意到。

所以我们关注主要情况：它可能在**未来**某些特定场景下会完全崩溃，但重启它仍然是有意义的。

///

你可能希望让负责重启应用的东西成为一个**外部组件**，因为到了那一步，运行着 Uvicorn 和 Python 的同一个应用已经崩溃了，因此在同一个应用的同一份代码中没有任何东西能对此做出处理。

### 自动重启的示例工具 { #example-tools-to-restart-automatically }

在大多数情况下，用于**启动时运行程序**的同一工具，也用于处理自动**重启**。

例如，可以通过以下方式处理：

* Docker
* Kubernetes
* Docker Compose
* Docker in Swarm Mode
* Systemd
* Supervisor
* 由云服务商在其服务中内部处理
* 其他...

## 复制 - 进程与内存 { #replication-processes-and-memory }

对于 FastAPI 应用，使用运行 Uvicorn 的 `fastapi` 命令等服务器程序时，在**一个进程**中运行一次就可以并发为多个客户端提供服务。

但在很多情况下，你会希望同时运行多个 worker 进程。

### 多进程 - Workers { #multiple-processes-workers }

如果你的客户端数量超过单个进程可以处理的数量（例如虚拟机不是太大），并且服务器 CPU 有**多个核心**，那么你可以让**多个进程**同时运行同一个应用，并在它们之间分发所有请求。

当你运行同一 API 程序的**多个进程**时，它们通常称为 **workers**。

### Worker 进程和端口 { #worker-processes-and-ports }

还记得在文档 [About HTTPS](https.md){.internal-link target=_blank} 中提到：在服务器中，一个端口和 IP 地址的组合只能由一个进程监听吗？

现在仍然如此。

因此，要能同时拥有**多个进程**，必须有一个**单个进程监听端口**，然后以某种方式将通信传递给每个 worker 进程。

### 每个进程的内存 { #memory-per-process }

当程序把内容加载到内存中时，例如将机器学习模型加载到变量中，或将大文件内容加载到变量中，这些都会消耗服务器的一部分**内存（RAM）**。

而多个进程通常**不共享任何内存**。这意味着每个正在运行的进程都有自己的内容、变量和内存。如果你的代码消耗了大量内存，**每个进程**都会消耗等量的内存。

### 服务器内存 { #server-memory }

例如，如果你的代码加载了一个大小为 **1 GB** 的机器学习模型，当你用一个进程运行 API 时，至少会消耗 1 GB RAM。如果你启动 **4 个进程**（4 个 worker），每个会消耗 1 GB RAM。因此总计你的 API 会消耗 **4 GB RAM**。

如果你的远程服务器或虚拟机只有 3 GB RAM，尝试加载超过 4 GB RAM 将导致问题。 🚨

### 多进程示例 { #multiple-processes-an-example }

在这个例子中，有一个 **Manager Process** 启动并控制两个 **Worker Processes**。

这个 Manager Process 很可能是监听该 IP 上**端口**的进程，并把所有通信传递给 worker 进程。

这些 worker 进程会运行你的应用，它们会执行主要计算来接收**请求**并返回**响应**，并加载你放进 RAM 变量中的任何内容。

<img src="/img/deployment/concepts/process-ram.drawio.svg">

当然，除了你的应用外，同一台机器可能还会运行**其他进程**。

一个有趣的细节是，每个进程使用的 **CPU 百分比**会随着时间发生很大**变化**，但**内存（RAM）**通常会或多或少保持**稳定**。

如果你有一个每次执行的计算量都差不多的 API，并且有很多客户端，那么 **CPU 利用率**很可能也会*保持稳定*（而不是不断快速上下波动）。

### 复制工具与策略示例 { #examples-of-replication-tools-and-strategies }

实现这一点有多种方法，我会在接下来的章节中告诉你更多具体策略，例如在谈论 Docker 和容器时。

需要考虑的主要限制是：必须有一个**单一**组件处理**公网 IP**上的**端口**。然后它必须有一种方式将通信**传递**给复制出来的**进程/workers**。

以下是一些可能的组合与策略：

* **Uvicorn** 配合 `--workers`
    * 一个 Uvicorn **进程管理器**会监听 **IP** 和 **端口**，并启动**多个 Uvicorn worker 进程**。
* **Kubernetes** 和其他分布式**容器系统**
    * **Kubernetes** 层中的某些组件会监听 **IP** 和 **端口**。复制方式是运行**多个容器**，每个容器运行**一个 Uvicorn 进程**。
* **云服务**替你处理这些
    * 云服务很可能会**替你处理复制**。它可能允许你定义**要运行的进程**，或要使用的**容器镜像**；无论哪种方式，最可能是**单个 Uvicorn 进程**，并由云服务负责复制它。

/// tip | 提示

如果这些关于**容器**、Docker 或 Kubernetes 的内容现在还不太好理解，也不用担心。

我会在未来的章节中讲解容器镜像、Docker、Kubernetes 等：[容器中的 FastAPI - Docker](docker.md){.internal-link target=_blank}。

///

## 启动前的前置步骤 { #previous-steps-before-starting }

在很多情况下，你希望在**启动**应用之前执行一些步骤。

例如，你可能想要运行**数据库迁移**。

但在大多数情况下，你只想执行这些步骤**一次**。

因此，你会希望在启动应用之前，有一个**单个进程**来执行这些**前置步骤**。

并且你必须确保运行前置步骤的是单个进程，*即使*之后你会为应用本身启动**多个进程**（多个 worker）。如果这些步骤由**多个进程**运行，它们会在**并行**执行时**重复**工作；如果这些步骤像数据库迁移一样比较敏感，它们还可能彼此发生冲突。

当然，也有一些情况，多次运行前置步骤也没有问题，那样就更容易处理。

/// tip | 提示

另外，请记住，根据你的设置，在某些情况下，你在启动应用之前**甚至可能不需要任何前置步骤**。

在这种情况下，你就不必担心这些。 🤷

///

### 前置步骤策略示例 { #examples-of-previous-steps-strategies }

这将**很大程度上取决于**你**部署系统的方式**，并且可能与启动程序、处理重启等方式相关。

以下是一些可能的思路：

* Kubernetes 中的 “Init Container”，在你的应用容器之前运行
* 一个 bash 脚本，运行前置步骤，然后启动你的应用
    * 你仍然需要一种方式来启动/重启*这个* bash 脚本、检测错误等。

/// tip | 提示

我会在未来的章节中给你更多使用容器来实现这一点的具体示例：[容器中的 FastAPI - Docker](docker.md){.internal-link target=_blank}。

///

## 资源利用率 { #resource-utilization }

你的服务器（们）是一种**资源**。你可以通过你的程序消耗或**利用** CPU 的计算时间，以及可用的 RAM 内存。

你希望消耗/利用多少系统资源？你可能会很容易觉得“不要太多”，但实际上，你可能希望在不崩溃的前提下**尽可能多地消耗**。

如果你为 3 台服务器付费，但只用了它们很少的 RAM 和 CPU，你可能在**浪费钱** 💸，也可能在**浪费服务器电力** 🌎，等等。

在这种情况下，可能更好的做法是只使用 2 台服务器，并以更高的比例使用它们的资源（CPU、内存、磁盘、网络带宽等）。

另一方面，如果你有 2 台服务器，并且使用了**100% 的 CPU 和 RAM**，那么在某个时刻，一个进程会请求更多内存，服务器就不得不把磁盘当作“内存”（可能会慢上数千倍），甚至**崩溃**。或者某个进程需要进行一些计算，并且必须等到 CPU 再次空闲。

在这种情况下，更好的做法是增加**一台额外的服务器**，并在其上运行一些进程，以便它们都有**足够的 RAM 和 CPU 时间**。

也有可能由于某种原因，你的 API 使用量出现**激增**。也许它爆火了，或者其他一些服务或机器人开始使用它。在这些情况下，你可能希望有额外资源以确保安全。

你可以设定一个**任意数字**作为目标，例如资源利用率在 **50% 到 90%** 之间。关键是，这些很可能是你想要衡量并用来调整部署的主要指标。

你可以使用 `htop` 等简单工具来查看服务器中 CPU 和 RAM 的使用情况，或每个进程使用的数量。或者你可以使用更复杂的监控工具，它们可能分布在多台服务器上等。

## 回顾 { #recap }

你在这里读到了一些在决定如何部署应用时需要记住的主要概念：

* 安全性 - HTTPS
* 启动时运行
* 重启
* 复制（运行的进程数量）
* 内存
* 启动前的前置步骤

理解这些想法以及如何应用它们，应该能给你必要的直觉，以便在配置和调整部署时做出任何决策。 🤓

在接下来的部分中，我会给你更多你可以遵循的、可能策略的具体示例。 🚀
