# Concepts de dÃ©ploiement { #deployments-concepts }

Lorsque vous dÃ©ployez une application **FastAPI**, ou en fait n'importe quel type de web API, il existe plusieurs concepts qui vous importent probablement, et en les utilisant vous pouvez trouver la maniÃ¨re la **plus appropriÃ©e** de **dÃ©ployer votre application**.

Parmi les concepts importants, on trouve :

* SÃ©curitÃ© - HTTPS
* ExÃ©cuter au dÃ©marrage
* RedÃ©marrages
* RÃ©plication (le nombre de processus en cours d'exÃ©cution)
* MÃ©moire
* Ã‰tapes prÃ©alables avant de dÃ©marrer

Nous allons voir comment ils affectent les **dÃ©ploiements**.

Au final, l'objectif ultime est de pouvoir **servir vos clients d'API** de maniÃ¨re **sÃ©curisÃ©e**, d'**Ã©viter les interruptions**, et d'utiliser les **ressources de calcul** (par exemple des serveurs/VM distants) aussi efficacement que possible. ğŸš€

Je vais vous en dire un peu plus ici sur ces **concepts**, ce qui devrait vous donner l'**intuition** nÃ©cessaire pour dÃ©cider comment dÃ©ployer votre API dans des environnements trÃ¨s diffÃ©rents, voire mÃªme dans des environnements **futurs** qui n'existent pas encore.

En tenant compte de ces concepts, vous serez en mesure **d'Ã©valuer et de concevoir** la meilleure faÃ§on de dÃ©ployer **vos propres API**.

Dans les chapitres suivants, je vous donnerai des **recettes concrÃ¨tes** pour dÃ©ployer des applications FastAPI.

Mais pour l'instant, voyons ces **idÃ©es conceptuelles** importantes. Ces concepts s'appliquent aussi Ã  tout autre type de web API. ğŸ’¡

## SÃ©curitÃ© - HTTPS { #security-https }

Dans le [chapitre prÃ©cÃ©dent Ã  propos de HTTPS](https.md){.internal-link target=_blank}, nous avons vu comment HTTPS fournit le chiffrement pour votre API.

Nous avons Ã©galement vu que HTTPS est normalement fourni par un composant **externe** Ã  votre serveur d'application, un **TLS Termination Proxy**.

Et il doit y avoir quelque chose chargÃ© de **renouveler les certificats HTTPS** ; cela peut Ãªtre le mÃªme composant ou quelque chose de diffÃ©rent.

### Exemples dâ€™outils pour HTTPS { #example-tools-for-https }

Parmi les outils que vous pourriez utiliser comme TLS Termination Proxy :

* Traefik
    * GÃ¨re automatiquement le renouvellement des certificats âœ¨
* Caddy
    * GÃ¨re automatiquement le renouvellement des certificats âœ¨
* Nginx
    * Avec un composant externe comme Certbot pour le renouvellement des certificats
* HAProxy
    * Avec un composant externe comme Certbot pour le renouvellement des certificats
* Kubernetes avec un Ingress Controller comme Nginx
    * Avec un composant externe comme cert-manager pour le renouvellement des certificats
* Pris en charge en interne par un fournisseur cloud dans le cadre de ses services (lisez ci-dessous ğŸ‘‡)

Une autre option consiste Ã  utiliser un **service cloud** qui fait davantage de travail, y compris la mise en place de HTTPS. Il peut avoir certaines restrictions ou vous facturer davantage, etc. Mais dans ce cas, vous n'auriez pas Ã  configurer vousâ€‘mÃªme un TLS Termination Proxy.

Je vous montrerai des exemples concrets dans les prochains chapitres.

---

Les concepts suivants Ã  considÃ©rer concernent tous le programme qui exÃ©cute votre API rÃ©elle (par ex. Uvicorn).

## Programme et processus { #program-and-process }

Nous allons beaucoup parler du Â« **processus** Â» en cours d'exÃ©cution, il est donc utile d'Ãªtre clair sur ce que cela signifie, et sur la diffÃ©rence avec le mot Â« **programme** Â».

### Qu'est-ce qu'un programme { #what-is-a-program }

Le mot **programme** est couramment utilisÃ© pour dÃ©crire plusieurs choses :

* Le **code** que vous Ã©crivez, les **fichiers Python**.
* Le **fichier** qui peut Ãªtre **exÃ©cutÃ©** par le systÃ¨me d'exploitation, par exemple : `python`, `python.exe` ou `uvicorn`.
* Un programme particulier lorsqu'il **s'exÃ©cute** sur le systÃ¨me d'exploitation, utilisant le CPU et stockant des choses en mÃ©moire. On appelle aussi cela un **processus**.

### Qu'est-ce qu'un processus { #what-is-a-process }

Le mot **processus** est normalement utilisÃ© de maniÃ¨re plus spÃ©cifique, en ne se rÃ©fÃ©rant qu'Ã  l'Ã©lÃ©ment qui s'exÃ©cute dans le systÃ¨me d'exploitation (comme dans le dernier point ciâ€‘dessus) :

* Un programme particulier lorsqu'il **s'exÃ©cute** sur le systÃ¨me d'exploitation.
    * Cela ne se rÃ©fÃ¨re ni au fichier, ni au code ; cela se rÃ©fÃ¨re **spÃ©cifiquement** Ã  l'Ã©lÃ©ment qui est **exÃ©cutÃ©** et gÃ©rÃ© par le systÃ¨me d'exploitation.
* N'importe quel programme, n'importe quel code, **ne peut faire des choses** que lorsqu'il est **exÃ©cutÃ©**. Donc, lorsqu'il y a un **processus en cours**.
* Le processus peut Ãªtre **arrÃªtÃ©** (ou Â« tuÃ© Â») par vous ou par le systÃ¨me d'exploitation. Ã€ ce momentâ€‘lÃ , il cesse de s'exÃ©cuter/d'Ãªtre exÃ©cutÃ©, et il **ne peut plus rien faire**.
* Chaque application que vous avez en cours d'exÃ©cution sur votre ordinateur a un processus derriÃ¨re elle, chaque programme lancÃ©, chaque fenÃªtre, etc. Et il y a normalement de nombreux processus exÃ©cutÃ©s **en mÃªme temps** tant qu'un ordinateur est allumÃ©.
* Il peut y avoir **plusieurs processus** du **mÃªme programme** exÃ©cutÃ©s simultanÃ©ment.

Si vous ouvrez le Â« gestionnaire des tÃ¢ches Â» ou le Â« moniteur systÃ¨me Â» (ou des outils similaires) de votre systÃ¨me d'exploitation, vous verrez nombre de ces processus en cours d'exÃ©cution.

Et, par exemple, vous verrez probablement qu'il y a plusieurs processus exÃ©cutant le mÃªme navigateur (Firefox, Chrome, Edge, etc.). Ils exÃ©cutent normalement un processus par onglet, plus quelques processus supplÃ©mentaires.

<img class="shadow" src="/img/deployment/concepts/image01.png">

---

Maintenant que nous connaissons la diffÃ©rence entre les termes **processus** et **programme**, continuons Ã  parler des dÃ©ploiements.

## ExÃ©cuter au dÃ©marrage { #running-on-startup }

Dans la plupart des cas, lorsque vous crÃ©ez une web API, vous voulez qu'elle **tourne en permanence**, sans interruption, afin que vos clients puissent toujours y accÃ©der. Bien sÃ»r, sauf si vous avez une raison spÃ©cifique de ne vouloir l'exÃ©cuter que dans certaines situations, mais la plupart du temps vous la voulez constamment en cours et **disponible**.

### Sur un serveur distant { #in-a-remote-server }

Lorsque vous configurez un serveur distant (un serveur cloud, une machine virtuelle, etc.), la chose la plus simple Ã  faire est d'utiliser `fastapi run` (qui utilise Uvicorn) ou quelque chose de similaire, manuellement, de la mÃªme maniÃ¨re que lorsque vous dÃ©veloppez en local.

Et cela fonctionnera et sera utile **pendant le dÃ©veloppement**.

Mais si votre connexion au serveur est coupÃ©e, le **processus en cours** va probablement s'arrÃªter.

Et si le serveur est redÃ©marrÃ© (par exemple aprÃ¨s des mises Ã  jour, ou des migrations chez le fournisseur cloud) vous **ne le remarquerez probablement pas**. Et Ã  cause de cela, vous ne saurez mÃªme pas que vous devez redÃ©marrer le processus manuellement. Ainsi, votre API restera tout simplement Ã  l'arrÃªt. ğŸ˜±

### Lancer automatiquement au dÃ©marrage { #run-automatically-on-startup }

En gÃ©nÃ©ral, vous voudrez probablement que le programme serveur (par ex. Uvicorn) soit dÃ©marrÃ© automatiquement au dÃ©marrage du serveur, et sans aucune **intervention humaine**, afin d'avoir en permanence un processus exÃ©cutant votre API (par ex. Uvicorn exÃ©cutant votre app FastAPI).

### Programme sÃ©parÃ© { #separate-program }

Pour y parvenir, vous aurez normalement un **programme sÃ©parÃ©** qui s'assure que votre application est lancÃ©e au dÃ©marrage. Et dans de nombreux cas, il s'assurera Ã©galement que d'autres composants ou applications sont Ã©galement lancÃ©s, par exemple une base de donnÃ©es.

### Exemples dâ€™outils pour lancer au dÃ©marrage { #example-tools-to-run-at-startup }

Voici quelques exemples d'outils capables de faire ce travail :

* Docker
* Kubernetes
* Docker Compose
* Docker en mode Swarm
* Systemd
* Supervisor
* Pris en charge en interne par un fournisseur cloud dans le cadre de ses services
* Autres ...

Je vous donnerai des exemples plus concrets dans les prochains chapitres.

## RedÃ©marrages { #restarts }

De la mÃªme maniÃ¨re que vous voulez vous assurer que votre application est lancÃ©e au dÃ©marrage, vous voulez probablement aussi vous assurer qu'elle est **redÃ©marrÃ©e** aprÃ¨s des Ã©checs.

### Nous faisons des erreurs { #we-make-mistakes }

Nous, humains, faisons des **erreurs**, tout le temps. Les logiciels ont presque *toujours* des **bugs** cachÃ©s Ã  diffÃ©rents endroits. ğŸ›

Et nous, dÃ©veloppeurs, continuons Ã  amÃ©liorer le code au fur et Ã  mesure que nous trouvons ces bugs et que nous implÃ©mentons de nouvelles fonctionnalitÃ©s (en ajoutant Ã©ventuellement de nouveaux bugs aussi ğŸ˜…).

### Petites erreurs gÃ©rÃ©es automatiquement { #small-errors-automatically-handled }

Lors de la crÃ©ation de web API avec FastAPI, s'il y a une erreur dans notre code, FastAPI la contiendra normalement Ã  la seule requÃªte qui a dÃ©clenchÃ© l'erreur. ğŸ›¡

Le client recevra un **500 Internal Server Error** pour cette requÃªte, mais l'application continuera de fonctionner pour les requÃªtes suivantes au lieu de simplement s'effondrer complÃ¨tement.

### Erreurs plus importantes - plantages { #bigger-errors-crashes }

NÃ©anmoins, il peut y avoir des cas oÃ¹ nous Ã©crivons du code qui **fait planter l'application entiÃ¨re**, faisant planter Uvicorn et Python. ğŸ’¥

Et malgrÃ© cela, vous ne voudrez probablement pas que l'application reste Ã  l'arrÃªt parce qu'il y a eu une erreur Ã  un endroit ; vous voudrez probablement qu'elle **continue de tourner**, au moins pour les *chemins d'accÃ¨s* qui ne sont pas cassÃ©s.

### RedÃ©marrer aprÃ¨s un plantage { #restart-after-crash }

Mais dans ces cas avec de trÃ¨s mauvaises erreurs qui font planter le **processus** en cours, vous voudrez un composant externe chargÃ© de **redÃ©marrer** le processus, au moins quelques fois ...

/// tip | Astuce

... Bien que si l'application entiÃ¨re **plante immÃ©diatement**, il n'est probablement pas logique de continuer Ã  la redÃ©marrer indÃ©finiment. Mais dans ces cas, vous le remarquerez probablement pendant le dÃ©veloppement, ou au moins juste aprÃ¨s le dÃ©ploiement.

Concentronsâ€‘nous donc sur les cas principaux, oÃ¹ elle pourrait planter entiÃ¨rement dans certaines situations particuliÃ¨res **Ã  l'avenir**, et oÃ¹ il est toujours logique de la redÃ©marrer.

///

Vous voudrez probablement que l'Ã©lÃ©ment chargÃ© de redÃ©marrer votre application soit un **composant externe**, car Ã  ce stade, l'application elleâ€‘mÃªme avec Uvicorn et Python a dÃ©jÃ  plantÃ©, donc il n'y a rien dans le mÃªme code de la mÃªme app qui pourrait y faire quoi que ce soit.

### Exemples dâ€™outils pour redÃ©marrer automatiquement { #example-tools-to-restart-automatically }

Dans la plupart des cas, le mÃªme outil qui est utilisÃ© pour **lancer le programme au dÃ©marrage** est Ã©galement utilisÃ© pour gÃ©rer les **redÃ©marrages** automatiques.

Par exemple, cela peut Ãªtre gÃ©rÃ© par :

* Docker
* Kubernetes
* Docker Compose
* Docker en mode Swarm
* Systemd
* Supervisor
* Pris en charge en interne par un fournisseur cloud dans le cadre de ses services
* Autres ...

## RÃ©plication - Processus et mÃ©moire { #replication-processes-and-memory }

Avec une application FastAPI, en utilisant un programme serveur comme la commande `fastapi` qui exÃ©cute Uvicorn, l'exÃ©cuter une fois dans **un processus** peut servir plusieurs clients simultanÃ©ment.

Mais dans de nombreux cas, vous voudrez exÃ©cuter plusieurs processus de travail en mÃªme temps.

### Multiples processus - Workers { #multiple-processes-workers }

Si vous avez plus de clients que ce qu'un seul processus peut gÃ©rer (par exemple si la machine virtuelle n'est pas trÃ¨s grande) et que vous avez **plusieurs cÅ“urs** dans le CPU du serveur, alors vous pouvez avoir **plusieurs processus** exÃ©cutant la mÃªme application simultanÃ©ment, et distribuer toutes les requÃªtes entre eux.

Quand vous exÃ©cutez **plusieurs processus** du mÃªme programme d'API, on les appelle couramment des **workers**.

### Processus workers et ports { #worker-processes-and-ports }

Rappelezâ€‘vous, d'aprÃ¨s les documents [Ã€ propos de HTTPS](https.md){.internal-link target=_blank}, qu'un seul processus peut Ã©couter une combinaison de port et d'adresse IP sur un serveur ?

C'est toujours vrai.

Donc, pour pouvoir avoir **plusieurs processus** en mÃªme temps, il doit y avoir un **seul processus Ã  l'Ã©coute sur un port** qui transmet ensuite la communication Ã  chaque processus worker d'une maniÃ¨re ou d'une autre.

### MÃ©moire par processus { #memory-per-process }

Maintenant, lorsque le programme charge des choses en mÃ©moire, par exemple, un modÃ¨le de machine learning dans une variable, ou le contenu d'un gros fichier dans une variable, tout cela **consomme une partie de la mÃ©moire (RAM)** du serveur.

Et plusieurs processus **ne partagent normalement pas de mÃ©moire**. Cela signifie que chaque processus en cours a ses propres Ã©lÃ©ments, variables et mÃ©moire. Et si vous consommez une grande quantitÃ© de mÃ©moire dans votre code, **chaque processus** consommera une quantitÃ© Ã©quivalente de mÃ©moire.

### MÃ©moire du serveur { #server-memory }

Par exemple, si votre code charge un modÃ¨le de Machine Learning de **1 Go**, lorsque vous exÃ©cutez un processus avec votre API, il consommera au moins 1 Go de RAM. Et si vous dÃ©marrez **4 processus** (4 workers), chacun consommera 1 Go de RAM. Donc au total, votre API consommera **4 Go de RAM**.

Et si votre serveur distant ou votre machine virtuelle n'a que 3 Go de RAM, essayer de charger plus de 4 Go de RAM posera problÃ¨me. ğŸš¨

### Multiples processus - Un exemple { #multiple-processes-an-example }

Dans cet exemple, il y a un **processus gestionnaire** qui dÃ©marre et contrÃ´le deux **processus workers**.

Ce processus gestionnaire serait probablement celui qui Ã©coute sur le **port** de l'IP. Et il transmettrait toute la communication aux processus workers.

Ces processus workers seraient ceux qui exÃ©cutent votre application, ils effectueraient les calculs principaux pour recevoir une **requÃªte** et renvoyer une **rÃ©ponse**, et ils chargeraient tout ce que vous mettez dans des variables en RAM.

<img src="/img/deployment/concepts/process-ram.drawio.svg">

Et bien sÃ»r, la mÃªme machine aurait probablement **d'autres processus** en cours d'exÃ©cution Ã©galement, en plus de votre application.

Un dÃ©tail intÃ©ressant est que le pourcentage de **CPU utilisÃ©** par chaque processus peut **varier** fortement dans le temps, mais la **mÃ©moire (RAM)** reste normalement plus ou moins **stable**.

Si vous avez une API qui effectue une quantitÃ© comparable de calculs Ã  chaque fois et que vous avez beaucoup de clients, alors l'**utilisation du CPU** sera probablement *Ã©galement stable* (au lieu de monter et descendre rapidement en permanence).

### Exemples dâ€™outils et de stratÃ©gies de rÃ©plication { #examples-of-replication-tools-and-strategies }

Il peut y avoir plusieurs approches pour y parvenir, et je vous en dirai plus sur des stratÃ©gies spÃ©cifiques dans les prochains chapitres, par exemple en parlant de Docker et des conteneurs.

La principale contrainte Ã  considÃ©rer est qu'il doit y avoir un **seul** composant gÃ©rant le **port** sur l'**IP publique**. Et il doit ensuite avoir un moyen de **transmettre** la communication aux **processus/workers** rÃ©pliquÃ©s.

Voici quelques combinaisons et stratÃ©gies possibles :

* **Uvicorn** avec `--workers`
    * Un **gestionnaire de processus** Uvicorn Ã©couterait sur l'**IP** et le **port**, et il dÃ©marrerait **plusieurs processus workers Uvicorn**.
* **Kubernetes** et autres systÃ¨mes **de conteneurs** distribuÃ©s
    * Quelque chose dans la couche **Kubernetes** Ã©couterait sur l'**IP** et le **port**. La rÃ©plication se ferait en ayant **plusieurs conteneurs**, chacun avec **un processus Uvicorn** en cours.
* **Services cloud** qui s'en chargent pour vous
    * Le service cloud **gÃ©rera probablement la rÃ©plication pour vous**. Il vous permettra Ã©ventuellement de dÃ©finir **un processus Ã  exÃ©cuter**, ou une **image de conteneur** Ã  utiliser ; dans tous les cas, ce sera trÃ¨s probablement **un seul processus Uvicorn**, et le service cloud sera chargÃ© de le rÃ©pliquer.

/// tip | Astuce

Ne vous inquiÃ©tez pas si certains de ces Ã©lÃ©ments concernant les **conteneurs**, Docker ou Kubernetes ne sont pas encore trÃ¨s clairs.

Je vous en dirai plus sur les images de conteneurs, Docker, Kubernetes, etc. dans un chapitre Ã  venir : [FastAPI dans des conteneurs - Docker](docker.md){.internal-link target=_blank}.

///

## Ã‰tapes prÃ©alables avant de dÃ©marrer { #previous-steps-before-starting }

Il existe de nombreux cas oÃ¹ vous souhaitez effectuer certaines Ã©tapes **avant de dÃ©marrer** votre application.

Par exemple, vous pourriez vouloir exÃ©cuter des **migrations de base de donnÃ©es**.

Mais dans la plupart des cas, vous voudrez effectuer ces Ã©tapes **une seule fois**.

Vous voudrez donc avoir un **processus unique** pour effectuer ces **Ã©tapes prÃ©alables**, avant de dÃ©marrer l'application.

Et vous devez vous assurer que c'est un processus unique qui exÃ©cute ces Ã©tapes prÃ©alables *mÃªme si*, ensuite, vous dÃ©marrez **plusieurs processus** (plusieurs workers) pour l'application elleâ€‘mÃªme. Si ces Ã©tapes Ã©taient exÃ©cutÃ©es par **plusieurs processus**, ils **dupliqueraient** le travail en l'exÃ©cutant **en parallÃ¨le**, et si les Ã©tapes Ã©taient dÃ©licates comme une migration de base de donnÃ©es, elles pourraient entrer en conflit les unes avec les autres.

Bien sÃ»r, il y a des cas oÃ¹ il n'y a aucun problÃ¨me Ã  exÃ©cuter les Ã©tapes prÃ©alables plusieurs fois ; dans ce cas, c'est beaucoup plus simple Ã  gÃ©rer.

/// tip | Astuce

Gardez aussi Ã  l'esprit que selon votre configuration, dans certains cas vous **n'aurez peutâ€‘Ãªtre mÃªme pas besoin d'Ã©tapes prÃ©alables** avant de dÃ©marrer votre application.

Dans ce cas, vous n'auriez pas Ã  vous soucier de tout cela. ğŸ¤·

///

### Exemples de stratÃ©gies pour les Ã©tapes prÃ©alables { #examples-of-previous-steps-strategies }

Cela **dÃ©pendra fortement** de la maniÃ¨re dont vous **dÃ©ployez votre systÃ¨me**, et sera probablement liÃ© Ã  votre maniÃ¨re de dÃ©marrer les programmes, de gÃ©rer les redÃ©marrages, etc.

Voici quelques idÃ©es possibles :

* Un Â« Init Container Â» dans Kubernetes qui s'exÃ©cute avant votre conteneur d'application
* Un script bash qui exÃ©cute les Ã©tapes prÃ©alables puis dÃ©marre votre application
    * Vous aurez toujours besoin d'un moyen de dÃ©marrer/redÃ©marrer *ce* script bash, de dÃ©tecter les erreurs, etc.

/// tip | Astuce

Je vous donnerai des exemples plus concrets pour faire cela avec des conteneurs dans un chapitre Ã  venir : [FastAPI dans des conteneurs - Docker](docker.md){.internal-link target=_blank}.

///

## Utilisation des ressources { #resource-utilization }

Votre ou vos serveurs constituent une **ressource** que vos programmes peuvent consommer ou **utiliser** : le temps de calcul des CPU et la mÃ©moire RAM disponible.

Quelle quantitÃ© des ressources systÃ¨me voulezâ€‘vous consommer/utiliser ? Il peut Ãªtre facile de penser Â« pas beaucoup Â», mais en rÃ©alitÃ©, vous voudrez probablement consommer **le plus possible sans planter**.

Si vous payez pour 3 serveurs mais que vous n'utilisez qu'un petit peu de leur RAM et CPU, vous **gaspillez probablement de l'argent** ğŸ’¸, et **gaspillez probablement l'Ã©lectricitÃ© des serveurs** ğŸŒ, etc.

Dans ce cas, il pourrait Ãªtre prÃ©fÃ©rable de n'avoir que 2 serveurs et d'utiliser un pourcentage plus Ã©levÃ© de leurs ressources (CPU, mÃ©moire, disque, bande passante rÃ©seau, etc.).

Ã€ l'inverse, si vous avez 2 serveurs et que vous utilisez **100 % de leur CPU et de leur RAM**, Ã  un moment donnÃ© un processus demandera plus de mÃ©moire, et le serveur devra utiliser le disque comme Â« mÃ©moire Â» (ce qui peut Ãªtre des milliers de fois plus lent), voire **planter**. Ou un processus pourrait avoir besoin de faire un calcul et devrait attendre que le CPU soit Ã  nouveau libre.

Dans ce cas, il serait prÃ©fÃ©rable d'obtenir **un serveur supplÃ©mentaire** et d'y exÃ©cuter certains processus afin qu'ils aient tous **suffisamment de RAM et de temps CPU**.

Il est Ã©galement possible que, pour une raison quelconque, vous ayez un **pic** d'utilisation de votre API. Peutâ€‘Ãªtre qu'elle devient virale, ou peutâ€‘Ãªtre que d'autres services ou bots commencent Ã  l'utiliser. Et vous voudrez peutâ€‘Ãªtre disposer de ressources supplÃ©mentaires pour Ãªtre en sÃ©curitÃ© dans ces cas.

Vous pouvez dÃ©finir un **chiffre arbitraire** comme cible, par exemple **entre 50 % et 90 %** d'utilisation des ressources. L'idÃ©e est que ce sont probablement les principaux Ã©lÃ©ments que vous voudrez mesurer et utiliser pour ajuster vos dÃ©ploiements.

Vous pouvez utiliser des outils simples comme `htop` pour voir le CPU et la RAM utilisÃ©s sur votre serveur ou la quantitÃ© utilisÃ©e par chaque processus. Ou vous pouvez utiliser des outils de supervision plus complexes, Ã©ventuellement distribuÃ©s sur plusieurs serveurs, etc.

## RÃ©capitulatif { #recap }

Vous venez de lire ici certains des principaux concepts que vous devrez probablement garder Ã  l'esprit lorsque vous dÃ©cidez comment dÃ©ployer votre application :

* SÃ©curitÃ© - HTTPS
* ExÃ©cuter au dÃ©marrage
* RedÃ©marrages
* RÃ©plication (le nombre de processus en cours d'exÃ©cution)
* MÃ©moire
* Ã‰tapes prÃ©alables avant de dÃ©marrer

Comprendre ces idÃ©es et comment les appliquer devrait vous donner l'intuition nÃ©cessaire pour prendre toutes les dÃ©cisions lors de la configuration et de l'ajustement de vos dÃ©ploiements. ğŸ¤“

Dans les sections suivantes, je vous donnerai des exemples plus concrets de stratÃ©gies possibles Ã  suivre. ğŸš€
