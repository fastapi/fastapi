# Concurrence et les mots-clÃ©s async et await

Cette page vise Ã  fournir des dÃ©tails sur la syntaxe `async def` pour les *fonctions de chemins* et quelques rappels sur le code asynchrone, la concurrence et le parallÃ©lisme.

## Vous Ãªtes pressÃ©s ?

<abbr title="'too long; didn't read' en anglais, ou 'trop long ; j'ai pas lu'"><strong>TL;DR :</strong></abbr>

Si vous utilisez des bibliothÃ¨ques tierces qui nÃ©cessitent d'Ãªtre appelÃ©es avec `await`, telles que :

```Python
results = await some_library()
```
Alors, dÃ©clarez vos *fonctions de chemins* avec `async def` comme ceci :

```Python hl_lines="2"
@app.get('/')
async def read_results():
    results = await some_library()
    return results
```

!!! note
    Vous pouvez uniquement utiliser `await` dans les fonctions crÃ©Ã©es avec `async def`.

---

Si vous utilisez une bibliothÃ¨que externe qui communique avec quelque chose (une BDD, une API, un systÃ¨me de fichiers, etc.) et qui ne supporte pas l'utilisation d'`await` (ce qui est actuellement le cas pour la majoritÃ© des bibliothÃ¨ques de BDD), alors dÃ©clarez vos *fonctions de chemin* normalement, avec le classique `def`, comme ceci :

```Python hl_lines="2"
@app.get('/')
def results():
    results = some_library()
    return results
```

---

Si votre application n'a pas Ã  communiquer avec une bibliothÃ¨que externe et pas Ã  attendre de rÃ©ponse, utilisez `async def`.

---

Si vous ne savez pas, utilisez seulement `def` comme vous le feriez habituellement.

---

**Note** : vous pouvez mÃ©langer `def` et `async def` dans vos *fonctions de chemin* autant que nÃ©cessaire, **FastAPI** saura faire ce qu'il faut avec.

Au final, peu importe le cas parmi ceux ci-dessus, **FastAPI** fonctionnera de maniÃ¨re asynchrone et sera extrÃªmement rapide.

Mais si vous suivez bien les instructions ci-dessus, alors **FastAPI** pourra effectuer quelques optimisations et ainsi amÃ©liorer les performances.

## DÃ©tails techniques

Les versions modernes de Python supportent le **code asynchrone** grÃ¢ce aux **"coroutines"** avec les syntaxes **`async` et `await`**.

Analysons les diffÃ©rentes parties de cette phrase dans les sections suivantes :

* **Code asynchrone**
* **`async` et `await`**
* **Coroutines**

## Code asynchrone

Faire du code asynchrone signifie que le langage ğŸ’¬ est capable de dire Ã  l'ordinateur / au programme ğŸ¤– qu'Ã  un moment du code, il ğŸ¤– devra attendre que *quelque chose d'autre* se termine autre part. Disons que ce *quelque chose d'autre* est appelÃ© "fichier-lent" ğŸ“.

Donc, pendant ce temps, l'ordinateur pourra effectuer d'autres tÃ¢ches, pendant que "fichier-lent" ğŸ“ se termine.

Ensuite l'ordinateur / le programme ğŸ¤– reviendra Ã  chaque fois qu'il en a la chance que ce soit parce qu'il attend Ã  nouveau, ou car il ğŸ¤– a fini tout le travail qu'il avait Ã  faire. Il ğŸ¤– regardera donc si les tÃ¢ches qu'il attend ont terminÃ© d'Ãªtre effectuÃ©es.

Ensuite, il ğŸ¤– prendra la premiÃ¨re tÃ¢che Ã  finir (disons, notre "fichier-lent" ğŸ“) et continuera Ã  faire avec cette derniÃ¨re ce qu'il Ã©tait censÃ©.

Ce "attendre quelque chose d'autre" fait gÃ©nÃ©ralement rÃ©fÃ©rence Ã  des opÃ©rations <abbr title="Input/Output ou EntrÃ©es et Sorties ">I/O</abbr> qui sont relativement "lentes" (comparÃ©es Ã  la vitesse du processeur et de la mÃ©moire RAM) telles qu'attendre que :

* de la donnÃ©e soit envoyÃ©e par le client Ã  travers le rÃ©seau
* de la donnÃ©e envoyÃ©e depuis votre programme soit reÃ§ue par le client Ã  travers le rÃ©seau
* le contenu d'un fichier sur le disque soit lu par le systÃ¨me et passÃ© Ã  votre programme
* le contenu que votre programme a passÃ© au systÃ¨me soit Ã©crit sur le disque
* une opÃ©ration effectuÃ©e Ã  distance par une API se termine
* une opÃ©ration en BDD se termine
* une requÃªte Ã  une BDD renvoie un rÃ©sultat
* etc.

Le temps d'exÃ©cution Ã©tant consommÃ© majoritairement par l'attente d'opÃ©rations <abbr title="Input/Output ou EntrÃ©es et Sorties ">I/O</abbr> on appelle ceci des opÃ©rations <a href="https://fr.wikipedia.org/wiki/I/O_bound" class="external-link" target="_blank">"I/O bound"</a>.

Ce concept se nomme l'"asynchronisme" car l'ordinateur / le programme n'a pas besoin d'Ãªtre "synchronisÃ©" avec la tÃ¢che, attendant le moment exact oÃ¹ cette derniÃ¨re se terminera en ne faisant rien, pour Ãªtre capable de rÃ©cupÃ©rer le rÃ©sultat de la tÃ¢che et l'utiliser dans la suite des opÃ©rations.

Ã€ la place, en Ã©tant "asynchrone", une fois terminÃ©e, une tÃ¢che peut lÃ©gÃ¨rement attendre (quelques microsecondes) que l'ordinateur / le programme finisse ce qu'il Ã©tait en train de faire, et revienne rÃ©cupÃ©rer le rÃ©sultat.

Pour parler de tÃ¢ches "synchrones" (en opposition Ã  "asynchrones"), on utilise souvent le terme "sÃ©quentiel", car l'ordinateur / le programme va effectuer toutes les Ã©tapes d'une tÃ¢che sÃ©quentiellement avant de passer Ã  une autre tÃ¢che, mÃªme si ces Ã©tapes impliquent de l'attente.

### Concurrence et Burgers

L'idÃ©e de code **asynchrone** dÃ©crite ci-dessus est parfois aussi appelÃ©e **"concurrence"**. Ce qui est diffÃ©rent du **"parallÃ©lisme"**.

La **concurrence** et le **parallÃ©lisme** sont tous deux liÃ©s Ã  l'idÃ©e de "diffÃ©rentes choses arrivant plus ou moins au mÃªme moment".

Mais les dÃ©tails entre la **concurrence** et le **parallÃ©lisme** diffÃ¨rent sur de nombreux points.

Pour expliquer la diffÃ©rence, voici une histoire de burgers :

#### Burgers concurrents

Vous amenez votre crush ğŸ˜ dans votre fast food ğŸ” favori, et faites la queue pendant que le serveur ğŸ’ prend les commandes des personnes devant vous.

Puis vient votre tour, vous commandez alors 2 magnifiques burgers ğŸ” pour votre crush ğŸ˜ et vous.

Vous payez ğŸ’¸.

Le serveur ğŸ’ dit quelque chose Ã  son collÃ¨gue dans la cuisine ğŸ‘¨â€ğŸ³ pour qu'il sache qu'il doit prÃ©parer vos burgers ğŸ” (bien qu'il soit dÃ©jÃ  en train de prÃ©parer ceux des clients prÃ©cÃ©dents).

Le serveur ğŸ’ vous donne le numÃ©ro assignÃ© Ã  votre commande.

Pendant que vous attendez, vous allez choisir une table avec votre crush ğŸ˜, vous discutez avec votre crush ğŸ˜ pendant un long moment (les burgers Ã©tant "magnifiques" ils sont trÃ¨s longs Ã  prÃ©parer âœ¨ğŸ”âœ¨).

Pendant que vous Ãªtes assis Ã  table, en attendant que les burgers ğŸ” soient prÃªts, vous pouvez passer ce temps Ã  admirer Ã  quel point votre crush ğŸ˜ est gÃ©niale, mignonne et intelligente âœ¨ğŸ˜âœ¨.

Pendant que vous discutez avec votre crush ğŸ˜, de temps en temps vous jetez un coup d'oeil au nombre affichÃ© au-dessus du comptoir pour savoir si c'est Ã  votre tour d'Ãªtre servis.

Jusqu'au moment oÃ¹ c'est (enfin) votre tour. Vous allez au comptoir, rÃ©cupÃ©rez vos burgers ğŸ” et revenez Ã  votre table.

Vous et votre crush ğŸ˜ mangez les burgers ğŸ” et passez un bon moment âœ¨.

---

Imaginez que vous Ãªtes l'ordinateur / le programme ğŸ¤– dans cette histoire.

Pendant que vous faites la queue, vous Ãªtre simplement inactif ğŸ˜´, attendant votre tour, ne faisant rien de "productif". Mais la queue est rapide car le serveur ğŸ’ prend seulement les commandes (et ne les prÃ©pare pas), donc tout va bien.

Ensuite, quand c'est votre tour, vous faites des actions "productives" ğŸ¤“, vous Ã©tudiez le menu, dÃ©cidez ce que vous voulez, demandez Ã  votre crush ğŸ˜ son choix, payez ğŸ’¸, vÃ©rifiez que vous utilisez la bonne carte de crÃ©dit, vÃ©rifiez que le montant dÃ©bitÃ© sur la carte est correct, vÃ©rifiez que la commande contient les bons produits, etc.

Mais ensuite, mÃªme si vous n'avez pas encore vos burgers ğŸ”, votre travail avec le serveur ğŸ’ est "en pause" â¸, car vous devez attendre ğŸ•™ que vos burgers soient prÃªts.

AprÃ¨s vous Ãªtre Ã©cartÃ© du comptoir et vous Ãªtre assis Ã  votre table avec le numÃ©ro de votre commande, vous pouvez tourner ğŸ”€ votre attention vers votre crush ğŸ˜, et "travailler" â¯ ğŸ¤“ lÃ -dessus. Vous Ãªtes donc Ã  nouveau en train de faire quelque chose de "productif" ğŸ¤“, vous flirtez avec votre crush ğŸ˜.

Puis le serveur ğŸ’ dit "J'ai fini de prÃ©parer les burgers" ğŸ” en mettant votre numÃ©ro sur l'affichage du comptoir, mais vous ne courrez pas immÃ©diatement au moment oÃ¹ votre numÃ©ro s'affiche. Vous savez que personne ne volera vos burgers ğŸ” car vous avez votre numÃ©ro et les autres clients ont le leur.

Vous attendez donc que votre crush ğŸ˜ finisse son histoire, souriez gentiment et dites que vous allez chercher les burgers â¸.

Pour finir vous allez au comptoir ğŸ”€, vers la tÃ¢che initiale qui est dÃ©sormais terminÃ©e â¯, rÃ©cupÃ©rez les burgers ğŸ”, remerciez le serveur et ramenez les burgers ğŸ” Ã  votre table. Ceci termine l'Ã©tape / la tÃ¢che d'interaction avec le comptoir â¹. Ce qui ensuite, crÃ©e une nouvelle tÃ¢che de "manger les burgers"  ğŸ”€ â¯, mais la prÃ©cÃ©dente, "rÃ©cupÃ©rer les burgers" est terminÃ©e â¹.

#### Burgers parallÃ¨les

Imaginons dÃ©sormais que ce ne sont pas des "burgers concurrents" mais des "burgers parallÃ¨les".

Vous allez avec votre crush ğŸ˜ dans un fast food ğŸ” parallÃ©lisÃ©.

Vous attendez pendant que plusieurs (disons 8) serveurs qui sont aussi des cuisiniers ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ prennent les commandes des personnes devant vous.

Chaque personne devant vous attend ğŸ•™ que son burger ğŸ” soit prÃªt avant de quitter le comptoir car chacun des 8 serveurs va lui-mÃªme prÃ©parer le burger directement avant de prendre la commande suivante.

Puis c'est enfin votre tour, vous commandez 2 magnifiques burgers ğŸ” pour vous et votre crush ğŸ˜.

Vous payez ğŸ’¸.

Le serveur va dans la cuisine ğŸ‘¨â€ğŸ³.

Vous attendez devant le comptoir afin que personne ne prenne vos burgers ğŸ” avant vous, vu qu'il n'y a pas de numÃ©ro de commande.

Vous et votre crush ğŸ˜ Ã©tant occupÃ©s Ã  vÃ©rifier que personne ne passe devant vous prendre vos burgers au moment oÃ¹ ils arriveront ğŸ•™, vous ne pouvez pas vous prÃ©occuper de votre crush ğŸ˜.

C'est du travail "synchrone", vous Ãªtre "synchronisÃ©s" avec le serveur/cuisinier ğŸ‘¨â€ğŸ³. Vous devez attendre ğŸ•™ et Ãªtre prÃ©sent au moment exact oÃ¹ le serveur/cuisinier ğŸ‘¨â€ğŸ³ finira les burgers ğŸ” et vous les donnera, sinon quelqu'un risque de vous les prendre.

Puis le serveur/cuisinier ğŸ‘¨â€ğŸ³ revient enfin avec vos burgers ğŸ”, aprÃ¨s un long moment d'attente ğŸ•™ devant le comptoir.

Vous prenez vos burgers ğŸ” et allez Ã  une table avec votre crush ğŸ˜

Vous les mangez, et vous avez terminÃ© ğŸ” â¹.

Durant tout ce processus, il n'y a presque pas eu de discussions ou de flirts car la plupart de votre temps Ã  Ã©tÃ© passÃ© Ã  attendre ğŸ•™ devant le comptoir ğŸ˜.

---

Dans ce scÃ©nario de burgers parallÃ¨les, vous Ãªtes un ordinateur / programme ğŸ¤– avec deux processeurs (vous et votre crush ğŸ˜) attendant ğŸ•™ Ã  deux et dÃ©diant votre attention ğŸ•™ Ã  "attendre devant le comptoir" pour une longue durÃ©e.

Le fast-food a 8 processeurs (serveurs/cuisiniers) ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³. Alors que le fast-food de burgers concurrents en avait 2 (un serveur et un cuisinier).

Et pourtant l'expÃ©rience finale n'est pas meilleure ğŸ˜.

---

C'est donc l'histoire Ã©quivalente parallÃ¨le pour les burgers ğŸ”.

Pour un exemple plus courant dans la "vie rÃ©elle", imaginez une banque.

Jusqu'Ã  rÃ©cemment, la plupart des banques avaient plusieurs caisses (et banquiers) ğŸ‘¨â€ğŸ’¼ğŸ‘¨â€ğŸ’¼ğŸ‘¨â€ğŸ’¼ğŸ‘¨â€ğŸ’¼ et une unique file d'attente ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™.

Tous les banquiers faisaient l'intÃ©gralitÃ© du travail avec chaque client avant de passer au suivant ğŸ‘¨â€ğŸ’¼â¯.

Et vous deviez attendre ğŸ•™ dans la file pendant un long moment ou vous perdiez votre place.

Vous n'auriez donc probablement pas envie d'amener votre crush ğŸ˜ avec vous Ã  la banque ğŸ¦.

#### Conclusion

Dans ce scÃ©nario des "burgers du fast-food avec votre crush", comme il y a beaucoup d'attente ğŸ•™, il est trÃ¨s logique d'avoir un systÃ¨me concurrent â¸ğŸ”€â¯.

Et c'est le cas pour la plupart des applications web.

Vous aurez de nombreux, nombreux utilisateurs, mais votre serveur attendra ğŸ•™ que leur connexion peu performante envoie des requÃªtes.

Puis vous attendrez ğŸ•™ de nouveau que leurs rÃ©ponses reviennent.

Cette "attente" ğŸ•™ se mesure en microsecondes, mais tout de mÃªme, en cumulÃ© cela fait beaucoup d'attente.

C'est pourquoi il est logique d'utiliser du code asynchrone â¸ğŸ”€â¯ pour des APIs web.

Ce type d'asynchronicitÃ© est ce qui a rendu NodeJS populaire (bien que NodeJS ne soit pas parallÃ¨le) et c'est la force du Go en tant que langage de programmation.

Et c'est le mÃªme niveau de performance que celui obtenu avec **FastAPI**.

Et comme on peut avoir du parallÃ©lisme et de l'asynchronicitÃ© en mÃªme temps, on obtient des performances plus hautes que la plupart des frameworks NodeJS et Ã©gales Ã  celles du Go, qui est un langage compilÃ© plus proche du C <a href="https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=query&l=zijmkf-1" class="external-link" target="_blank">(tout Ã§a grÃ¢ce Ã  Starlette)</a>.

### Est-ce que la concurrence est mieux que le parallÃ©lisme ?

Nope ! C'est Ã§a la morale de l'histoire.

La concurrence est diffÃ©rente du parallÃ©lisme. C'est mieux sur des scÃ©narios **spÃ©cifiques** qui impliquent beaucoup d'attente. Ã€ cause de Ã§a, c'est gÃ©nÃ©ralement bien meilleur que le parallÃ©lisme pour le dÃ©veloppement d'applications web. Mais pas pour tout.

Donc pour Ã©quilibrer tout Ã§a, imaginez l'histoire suivante :

> Vous devez nettoyer une grande et sale maison.

*Oui, c'est toute l'histoire*.

---

Il n'y a plus d'attente ğŸ•™ nulle part, juste beaucoup de travail Ã  effectuer, dans diffÃ©rentes piÃ¨ces de la maison.

Vous pourriez diviser en diffÃ©rentes sections comme avec les burgers, d'abord le salon, puis la cuisine, etc. Mais vous n'attendez ğŸ•™ rien, vous ne faites que nettoyer et nettoyer, la sÃ©paration en sections ne changerait rien au final.

Cela prendrait autant de temps pour finir avec ou sans sections (concurrence) et vous auriez effectuÃ© la mÃªme quantitÃ© de travail.

Mais dans ce cas, si pouviez amener 8 ex-serveurs/cuisiniers/devenus-nettoyeurs ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³, et que chacun d'eux (plus vous) pouvait prendre une zone de la maison pour la nettoyer, vous pourriez faire tout le travail en parallÃ¨le, et finir plus tÃ´t.

Dans ce scÃ©nario, chacun des nettoyeurs (vous y compris) serait un processeur, faisant sa partie du travail.

Et comme la plupart du temps d'exÃ©cution est pris par du "vrai" travail (et non de l'attente), et que le travail dans un ordinateur est fait par un <abbr title="Central Processing Unit">CPU</abbr>, ce sont des problÃ¨mes dits "CPU bound".

---

Des exemples communs d'opÃ©rations "CPU bounds" sont les procÃ©dÃ©s qui requiÃ¨rent des traitements mathÃ©matiques complexes.

Par exemple :

* Traitements d'**audio** et d'**images**.
* La **vision par ordinateur** : une image est composÃ©e de millions de pixels, chaque pixel ayant 3 valeurs / couleurs, les traiter tous va nÃ©cessiter d'effectuer des traitements sur chaque pixel, et de prÃ©fÃ©rence tous en mÃªme temps.
* L'apprentissage automatique (ou **Machine Learning**) : cela nÃ©cessite de nombreuses multiplications de matrices et vecteurs. Imaginez une Ã©norme feuille de calcul remplie de nombres que vous multiplierez entre eux tous au mÃªme moment.
* L'apprentissage profond (ou **Deep Learning**) : est un sous-domaine du **Machine Learning**, donc les mÃªmes raisons s'appliquent. Avec la diffÃ©rence qu'il n'y a pas une unique feuille de calcul de nombres Ã  multiplier, mais une Ã©norme quantitÃ© d'entre elles, et dans de nombreux cas, on utilise un processeur spÃ©cial pour construire et / ou utiliser ces modÃ¨les.

### Concurrence + ParallÃ©lisme : Web + Machine Learning

Avec **FastAPI** vous pouvez bÃ©nÃ©ficier de la concurrence qui est trÃ¨s courante en developement web (c'est l'attrait principal de NodeJS).

Mais vous pouvez aussi profiter du parallÃ©lisme et multiprocessing afin de gÃ©rer des charges **CPU bound** qui sont rÃ©currentes dans les systÃ¨mes de *Machine Learning*.

Ã‡a, ajoutÃ© au fait que Python soit le langage le plus populaire pour la **Data Science**, le **Machine Learning** et surtout le **Deep Learning**, font de **FastAPI** un trÃ¨s bon choix pour les APIs et applications de **Data Science** / **Machine Learning**.

Pour comprendre comment mettre en place ce parallÃ©lisme en production, allez lire la section [DÃ©ploiement](deployment/index.md){.internal-link target=_blank}.

## `async` et `await`

Les versions modernes de Python ont une maniÃ¨re trÃ¨s intuitive de dÃ©finir le code asynchrone, tout en gardant une apparence de code "sÃ©quentiel" classique en laissant Python faire l'attente pour vous au bon moment.

Pour une opÃ©ration qui nÃ©cessite de l'attente avant de donner un rÃ©sultat et qui supporte ces nouvelles fonctionnalitÃ©s Python, vous pouvez l'utiliser comme tel :

```Python
burgers = await get_burgers(2)
```

Le mot-clÃ© important ici est `await`. Il informe Python qu'il faut attendre â¸ que `get_burgers(2)` finisse d'effectuer ses opÃ©rations ğŸ•™ avant de stocker les rÃ©sultats dans la variable `burgers`. GrÃ¢ce Ã  cela, Python saura qu'il peut aller effectuer d'autres opÃ©rations ğŸ”€ â¯ pendant ce temps (comme par exemple recevoir une autre requÃªte).

Pour que `await` fonctionne, il doit Ãªtre placÃ© dans une fonction qui supporte l'asynchronicitÃ©. Pour que Ã§a soit le cas, il faut dÃ©clarer cette derniÃ¨re avec `async def` :

```Python hl_lines="1"
async def get_burgers(number: int):
    # OpÃ©rations asynchrones pour crÃ©er les burgers
    return burgers
```

...et non `def` :

```Python hl_lines="2"
# Ceci n'est pas asynchrone
def get_sequential_burgers(number: int):
    # OpÃ©rations asynchrones pour crÃ©er les burgers
    return burgers
```

Avec `async def`, Python sait que dans cette fonction il doit prendre en compte les expressions `await`, et qu'il peut mettre en pause â¸ l'exÃ©cution de la fonction pour aller faire autre chose ğŸ”€ avant de revenir.

Pour appeler une fonction dÃ©finie avec `async def`, vous devez utiliser `await`. Donc ceci ne marche pas :

```Python
# Ceci ne fonctionne pas, car get_burgers a Ã©tÃ© dÃ©fini avec async def
burgers = get_burgers(2)
```

---

Donc, si vous utilisez une bibliothÃ¨que qui nÃ©cessite que ses fonctions soient appelÃ©es avec `await`, vous devez dÃ©finir la *fonction de chemin* en utilisant `async def` comme dans :

```Python hl_lines="2-3"
@app.get('/burgers')
async def read_burgers():
    burgers = await get_burgers(2)
    return burgers
```

### Plus de dÃ©tails techniques

Vous avez donc compris que `await` peut seulement Ãªtre utilisÃ© dans des fonctions dÃ©finies avec `async def`.

Mais en mÃªme temps, les fonctions dÃ©finies avec `async def` doivent Ãªtre appelÃ©es avec `await` et donc dans des fonctions dÃ©finies elles aussi avec `async def`.

Vous avez donc remarquÃ© ce paradoxe d'oeuf et de la poule, comment appelle-t-on la premiÃ¨re fonction `async` ?

Si vous utilisez **FastAPI**, pas besoin de vous en inquiÃ©ter, car cette "premiÃ¨re" fonction sera votre *fonction de chemin* ; et **FastAPI** saura comment arriver au rÃ©sultat attendu.

Mais si vous utilisez `async` / `await` sans **FastAPI**, <a href="https://docs.python.org/3/library/asyncio-task.html#coroutine" class="external-link" target="_blank">allez jetez un coup d'oeil Ã  la documentation officielle de Python</a>.

### Autres formes de code asynchrone

L'utilisation d'`async` et `await` est relativement nouvelle dans ce langage.

Mais cela rend la programmation asynchrone bien plus simple.

Cette mÃªme syntaxe (ou presque) Ã©tait aussi incluse dans les versions modernes de Javascript (dans les versions navigateur et NodeJS).

Mais avant Ã§a, gÃ©rer du code asynchrone Ã©tait bien plus complexe et difficile.

Dans les versions prÃ©cÃ©dentes de Python, vous auriez utilisÃ© des *threads* ou <a href="https://www.gevent.org/" class="external-link" target="_blank">Gevent</a>.  Mais le code aurait Ã©tÃ© bien plus difficile Ã  comprendre, dÃ©bugger, et concevoir.

Dans les versions prÃ©cÃ©dentes de Javascript NodeJS / Navigateur, vous auriez utilisÃ© des "callbacks". Menant potentiellement Ã  ce que l'on appelle <a href="http://callbackhell.com/" class="external-link" target="_blank">le "callback hell"</a>.


## Coroutines

**Coroutine** est juste un terme Ã©laborÃ© pour dÃ©signer ce qui est retournÃ© par une fonction dÃ©finie avec `async def`. Python sait que c'est comme une fonction classique qui va dÃ©marrer Ã  un moment et terminer Ã  un autre, mais qu'elle peut aussi Ãªtre mise en pause â¸, du moment qu'il y a un `await` dans son contenu.

Mais toutes ces fonctionnalitÃ©s d'utilisation de code asynchrone avec `async` et `await` sont souvent rÃ©sumÃ©es comme l'utilisation des *coroutines*. On peut comparer cela Ã  la principale fonctionnalitÃ© clÃ© de Go, les "Goroutines".

## Conclusion

Reprenons la phrase du dÃ©but de la page :

> Les versions modernes de Python supportent le **code asynchrone** grÃ¢ce aux **"coroutines"** avec les syntaxes **`async` et `await`**.

Ceci devrait Ãªtre plus comprÃ©hensible dÃ©sormais. âœ¨

Tout ceci est donc ce qui donne sa force Ã  **FastAPI** (Ã  travers Starlette) et lui permet d'avoir des performances aussi impressionnantes.

## DÃ©tails trÃ¨s techniques

!!! warning "Attention !"
    Vous pouvez probablement ignorer cela.

    Ce sont des dÃ©tails trÃ¨s poussÃ©s sur comment **FastAPI** fonctionne en arriÃ¨re-plan.

    Si vous avez de bonnes connaissances techniques (coroutines, threads, code bloquant, etc.) et Ãªtes curieux de comment **FastAPI** gÃ¨re `async def` versus le `def` classique, cette partie est faite pour vous.

### Fonctions de chemin

Quand vous dÃ©clarez une *fonction de chemin* avec un `def` normal et non `async def`, elle est exÃ©cutÃ©e dans un groupe de threads (threadpool) externe qui est ensuite attendu, plutÃ´t que d'Ãªtre appelÃ©e directement (car cela bloquerait le serveur).

Si vous venez d'un autre framework asynchrone qui ne fonctionne pas comme de la faÃ§on dÃ©crite ci-dessus et que vous Ãªtes habituÃ©s Ã  dÃ©finir des *fonctions de chemin* basiques avec un simple `def` pour un faible gain de performance (environ 100 nanosecondes), veuillez noter que dans **FastAPI**, l'effet serait plutÃ´t contraire. Dans ces cas-lÃ , il vaut mieux utiliser `async def` Ã  moins que votre *fonction de chemin* utilise du code qui effectue des opÃ©rations <abbr title="Input/Output ou EntrÃ©es et Sorties ">I/O</abbr> bloquantes.

Au final, dans les deux situations, il est fort probable que **FastAPI** soit tout de mÃªme [plus rapide](/#performance){.internal-link target=_blank} que (ou au moins de vitesse Ã©gale Ã ) votre framework prÃ©cÃ©dent.

### DÃ©pendances

La mÃªme chose s'applique aux dÃ©pendances. Si une dÃ©pendance est dÃ©finie avec `def` plutÃ´t que `async def`, elle est exÃ©cutÃ©e dans la threadpool externe.

### Sous-dÃ©pendances

Vous pouvez avoir de multiples dÃ©pendances et sous-dÃ©pendances dÃ©pendant les unes des autres (en tant que paramÃ¨tres de la dÃ©finition de la *fonction de chemin*), certaines crÃ©Ã©es avec `async def` et d'autres avec `def`. Cela fonctionnerait aussi, et celles dÃ©finies avec un simple `def` seraient exÃ©cutÃ©es sur un thread externe (venant de la threadpool) plutÃ´t que d'Ãªtre "attendues".

### Autres fonctions utilitaires

Toute autre fonction utilitaire que vous appelez directement peut Ãªtre crÃ©Ã©e avec un classique `def` ou avec `async def` et **FastAPI** n'aura pas d'impact sur la faÃ§on dont vous l'appelez.

Contrairement aux fonctions que **FastAPI** appelle pour vous : les *fonctions de chemin* et dÃ©pendances.

Si votre fonction utilitaire est une fonction classique dÃ©finie avec `def`, elle sera appelÃ©e directement (telle qu'Ã©crite dans votre code), pas dans une threadpool, si la fonction est dÃ©finie avec `async def` alors vous devrez attendre (avec `await`) que cette fonction se termine avant de passer Ã  la suite du code.

---

Encore une fois, ce sont des dÃ©tails trÃ¨s techniques qui peuvent Ãªtre utiles si vous venez ici les chercher.

Sinon, les instructions de la section <a href="#vous-etes-presses">Vous Ãªtes pressÃ©s ?</a> ci-dessus sont largement suffisantes.
