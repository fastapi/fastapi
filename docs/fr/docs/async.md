# Concurrence et async / await { #concurrency-and-async-await }

DÃ©tails sur la syntaxe `async def` pour les *fonctions de chemin d'accÃ¨s* et quelques rappels sur le code asynchrone, la concurrence et le parallÃ©lisme.

## Vous Ãªtes pressÃ©s ? { #in-a-hurry }

<abbr title="too long; didn't read - trop long ; pas lu"><strong>TL;DR :</strong></abbr>

Si vous utilisez des bibliothÃ¨ques tierces qui nÃ©cessitent d'Ãªtre appelÃ©es avec `await`, telles que :

```Python
results = await some_library()
```

Alors, dÃ©clarez vos *fonctions de chemin d'accÃ¨s* avec `async def` comme ceci :

```Python hl_lines="2"
@app.get('/')
async def read_results():
    results = await some_library()
    return results
```

/// note | Remarque

Vous pouvez uniquement utiliser `await` dans les fonctions crÃ©Ã©es avec `async def`.

///

---

Si vous utilisez une bibliothÃ¨que externe qui communique avec quelque chose (une base de donnÃ©es, une API, le systÃ¨me de fichiers, etc.) et qui ne supporte pas l'utilisation d'`await` (ce qui est actuellement le cas pour la majoritÃ© des bibliothÃ¨ques de base de donnÃ©es), alors dÃ©clarez vos *fonctions de chemin d'accÃ¨s* normalement, avec le classique `def`, comme ceci :

```Python hl_lines="2"
@app.get('/')
def results():
    results = some_library()
    return results
```

---

Si votre application n'a pas Ã  communiquer avec une autre chose et Ã  attendre sa rÃ©ponse, utilisez `async def`, mÃªme si vous n'avez pas besoin d'utiliser `await` Ã  l'intÃ©rieur.

---

Si vous ne savez pas, utilisez seulement `def` comme vous le feriez habituellement.

---

Note : vous pouvez mÃ©langer `def` et `async def` dans vos *fonctions de chemin d'accÃ¨s* autant que nÃ©cessaire, et dÃ©finir chacune avec lâ€™option la plus adaptÃ©e pour vous. FastAPI fera ce qu'il faut avec elles.

Au final, peu importe le cas parmi ceux ci-dessus, FastAPI fonctionnera de maniÃ¨re asynchrone et sera extrÃªmement rapide.

Mais si vous suivez bien les instructions ci-dessus, il pourra effectuer quelques optimisations et ainsi amÃ©liorer les performances.

## DÃ©tails techniques { #technical-details }

Les versions modernes de Python supportent le **code asynchrone** grÃ¢ce aux **Â« coroutines Â»** avec les syntaxes **`async` et `await`**.

Analysons les diffÃ©rentes parties de cette phrase dans les sections suivantes :

* **Code asynchrone**
* **`async` et `await`**
* **Coroutines**

## Code asynchrone { #asynchronous-code }

Faire du code asynchrone signifie que le langage ğŸ’¬ est capable de dire Ã  l'ordinateur / au programme ğŸ¤– qu'Ã  un moment du code, il ğŸ¤– devra attendre que *quelque chose d'autre* se termine autre part. Disons que ce *quelque chose d'autre* est appelÃ© Â« slow-file Â» ğŸ“.

Donc, pendant ce temps, l'ordinateur pourra effectuer d'autres tÃ¢ches, pendant que Â« slow-file Â» ğŸ“ se termine.

Ensuite l'ordinateur / le programme ğŸ¤– reviendra Ã  chaque fois qu'il en a la chance que ce soit parce qu'il attend Ã  nouveau, ou car il ğŸ¤– a fini tout le travail qu'il avait Ã  faire. Il ğŸ¤– regardera donc si les tÃ¢ches qu'il attend ont terminÃ© d'Ãªtre effectuÃ©es.

Ensuite, il ğŸ¤– prendra la premiÃ¨re tÃ¢che Ã  finir (disons, notre Â« slow-file Â» ğŸ“) et continuera Ã  faire avec cette derniÃ¨re ce qu'il Ã©tait censÃ©.

Ce Â« attendre quelque chose d'autre Â» fait gÃ©nÃ©ralement rÃ©fÃ©rence Ã  des opÃ©rations <abbr title="Input and Output - EntrÃ©es et sorties">I/O</abbr> qui sont relativement Â« lentes Â» (comparÃ©es Ã  la vitesse du processeur et de la mÃ©moire RAM) telles qu'attendre que :

* de la donnÃ©e soit envoyÃ©e par le client Ã  travers le rÃ©seau
* de la donnÃ©e envoyÃ©e depuis votre programme soit reÃ§ue par le client Ã  travers le rÃ©seau
* le contenu d'un fichier sur le disque soit lu par le systÃ¨me et passÃ© Ã  votre programme
* le contenu que votre programme a passÃ© au systÃ¨me soit Ã©crit sur le disque
* une opÃ©ration effectuÃ©e Ã  distance par une API se termine
* une opÃ©ration en base de donnÃ©es se termine
* une requÃªte Ã  une base de donnÃ©es renvoie un rÃ©sultat
* etc.

Le temps d'exÃ©cution Ã©tant consommÃ© majoritairement par l'attente d'opÃ©rations <abbr title="Input and Output - EntrÃ©es et sorties">I/O</abbr>, on appelle ceci des opÃ©rations Â« I/O bound Â».

Ce concept se nomme Â« asynchrone Â» car l'ordinateur / le programme n'a pas besoin d'Ãªtre Â« synchronisÃ© Â» avec la tÃ¢che, attendant le moment exact oÃ¹ cette derniÃ¨re se terminera en ne faisant rien, pour Ãªtre capable de rÃ©cupÃ©rer le rÃ©sultat de la tÃ¢che et l'utiliser dans la suite des opÃ©rations.

Ã€ la place, en Ã©tant Â« asynchrone Â», une fois terminÃ©e, une tÃ¢che peut lÃ©gÃ¨rement attendre (quelques microsecondes) que l'ordinateur / le programme finisse ce qu'il Ã©tait en train de faire, et revienne rÃ©cupÃ©rer le rÃ©sultat.

Pour parler de tÃ¢ches Â« synchrones Â» (en opposition Ã  Â« asynchrones Â»), on utilise souvent le terme Â« sÃ©quentiel Â», car l'ordinateur / le programme va effectuer toutes les Ã©tapes d'une tÃ¢che sÃ©quentiellement avant de passer Ã  une autre tÃ¢che, mÃªme si ces Ã©tapes impliquent de l'attente.

### Concurrence et Burgers { #concurrency-and-burgers }

L'idÃ©e de code **asynchrone** dÃ©crite ci-dessus est parfois aussi appelÃ©e **Â« concurrence Â»**. Ce qui est diffÃ©rent du **Â« parallÃ©lisme Â»**.

La **concurrence** et le **parallÃ©lisme** sont tous deux liÃ©s Ã  l'idÃ©e de Â« diffÃ©rentes choses arrivant plus ou moins au mÃªme moment Â».

Mais les dÃ©tails entre la **concurrence** et le **parallÃ©lisme** diffÃ¨rent sur de nombreux points.

Pour expliquer la diffÃ©rence, voici une histoire de burgers :

### Burgers concurrents { #concurrent-burgers }

Vous amenez votre crush ğŸ˜ dans votre fast food ğŸ” favori, et faites la queue pendant que le serveur ğŸ’ prend les commandes des personnes devant vous.

<img src="/img/async/concurrent-burgers/concurrent-burgers-01.png" class="illustration">

Puis vient votre tour, vous commandez alors 2 magnifiques burgers ğŸ” pour votre crush ğŸ˜ et vous.

<img src="/img/async/concurrent-burgers/concurrent-burgers-02.png" class="illustration">

Le serveur ğŸ’ dit quelque chose Ã  son collÃ¨gue dans la cuisine ğŸ‘¨â€ğŸ³ pour qu'il sache qu'il doit prÃ©parer vos burgers ğŸ” (bien qu'il soit dÃ©jÃ  en train de prÃ©parer ceux des clients prÃ©cÃ©dents).

<img src="/img/async/concurrent-burgers/concurrent-burgers-03.png" class="illustration">

Vous payez ğŸ’¸.

Le serveur ğŸ’ vous donne le numÃ©ro assignÃ© Ã  votre commande.

<img src="/img/async/concurrent-burgers/concurrent-burgers-04.png" class="illustration">

Pendant que vous attendez, vous allez choisir une table avec votre crush ğŸ˜, vous discutez avec votre crush ğŸ˜ pendant un long moment (les burgers Ã©tant Â« magnifiques Â» ils sont trÃ¨s longs Ã  prÃ©parer âœ¨ğŸ”âœ¨).

Pendant que vous Ãªtes assis Ã  table, en attendant que les burgers ğŸ” soient prÃªts, vous pouvez passer ce temps Ã  admirer Ã  quel point votre crush ğŸ˜ est gÃ©niale, mignonne et intelligente âœ¨ğŸ˜âœ¨.

<img src="/img/async/concurrent-burgers/concurrent-burgers-05.png" class="illustration">

Pendant que vous discutez avec votre crush ğŸ˜, de temps en temps vous jetez un coup dâ€™Å“il au nombre affichÃ© au-dessus du comptoir pour savoir si c'est Ã  votre tour d'Ãªtre servis.

Jusqu'au moment oÃ¹ c'est (enfin) votre tour. Vous allez au comptoir, rÃ©cupÃ©rez vos burgers ğŸ” et revenez Ã  votre table.

<img src="/img/async/concurrent-burgers/concurrent-burgers-06.png" class="illustration">

Vous et votre crush ğŸ˜ mangez les burgers ğŸ” et passez un bon moment âœ¨.

<img src="/img/async/concurrent-burgers/concurrent-burgers-07.png" class="illustration">

/// info

Illustrations proposÃ©es par <a href="https://www.instagram.com/ketrinadrawsalot" class="external-link" target="_blank">Ketrina Thompson</a>. ğŸ¨

///

---

Imaginez que vous Ãªtes l'ordinateur / le programme ğŸ¤– dans cette histoire.

Pendant que vous faites la queue, vous Ãªtre simplement inactif ğŸ˜´, attendant votre tour, ne faisant rien de Â« productif Â». Mais la queue est rapide car le serveur ğŸ’ prend seulement les commandes (et ne les prÃ©pare pas), donc tout va bien.

Ensuite, quand c'est votre tour, vous faites des actions Â« productives Â» ğŸ¤“, vous Ã©tudiez le menu, dÃ©cidez ce que vous voulez, demandez Ã  votre crush ğŸ˜ son choix, payez ğŸ’¸, vÃ©rifiez que vous utilisez la bonne carte de crÃ©dit, vÃ©rifiez que le montant dÃ©bitÃ© sur la carte est correct, vÃ©rifiez que la commande contient les bons produits, etc.

Mais ensuite, mÃªme si vous n'avez pas encore vos burgers ğŸ”, votre travail avec le serveur ğŸ’ est Â« en pause Â» â¸, car vous devez attendre ğŸ•™ que vos burgers soient prÃªts.

AprÃ¨s vous Ãªtre Ã©cartÃ© du comptoir et vous Ãªtre assis Ã  votre table avec le numÃ©ro de votre commande, vous pouvez tourner ğŸ”€ votre attention vers votre crush ğŸ˜, et Â« travailler Â» â¯ ğŸ¤“ lÃ -dessus. Vous Ãªtes donc Ã  nouveau en train de faire quelque chose de Â« productif Â» ğŸ¤“, vous flirtez avec votre crush ğŸ˜.

Puis le serveur ğŸ’ dit Â« J'ai fini de prÃ©parer les burgers Â» ğŸ” en mettant votre numÃ©ro sur l'affichage du comptoir, mais vous ne courez pas immÃ©diatement au moment oÃ¹ votre numÃ©ro s'affiche. Vous savez que personne ne volera vos burgers ğŸ” car vous avez votre numÃ©ro et les autres clients ont le leur.

Vous attendez donc que votre crush ğŸ˜ finisse son histoire, souriez gentiment et dites que vous allez chercher les burgers â¸.

Pour finir vous allez au comptoir ğŸ”€, vers la tÃ¢che initiale qui est dÃ©sormais terminÃ©e â¯, rÃ©cupÃ©rez les burgers ğŸ”, remerciez le serveur et ramenez les burgers ğŸ” Ã  votre table. Ceci termine l'Ã©tape / la tÃ¢che d'interaction avec le comptoir â¹. Ce qui ensuite, crÃ©e une nouvelle tÃ¢che de Â« manger les burgers Â» ğŸ”€ â¯, mais la prÃ©cÃ©dente, Â« rÃ©cupÃ©rer les burgers Â» est terminÃ©e â¹.

### Burgers parallÃ¨les { #parallel-burgers }

Imaginons dÃ©sormais que ce ne sont pas des Â« burgers concurrents Â» mais des Â« burgers parallÃ¨les Â».

Vous allez avec votre crush ğŸ˜ dans un fast food ğŸ” parallÃ©lisÃ©.

Vous attendez pendant que plusieurs (disons 8) serveurs qui sont aussi des cuisiniers ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ prennent les commandes des personnes devant vous.

Chaque personne devant vous attend ğŸ•™ que son burger ğŸ” soit prÃªt avant de quitter le comptoir car chacun des 8 serveurs va lui-mÃªme prÃ©parer le burger directement avant de prendre la commande suivante.

<img src="/img/async/parallel-burgers/parallel-burgers-01.png" class="illustration">

Puis c'est enfin votre tour, vous commandez 2 magnifiques burgers ğŸ” pour vous et votre crush ğŸ˜.

Vous payez ğŸ’¸.

<img src="/img/async/parallel-burgers/parallel-burgers-02.png" class="illustration">

Le serveur va dans la cuisine ğŸ‘¨â€ğŸ³.

Vous attendez devant le comptoir afin que personne ne prenne vos burgers ğŸ” avant vous, vu qu'il n'y a pas de numÃ©ro de commande.

<img src="/img/async/parallel-burgers/parallel-burgers-03.png" class="illustration">

Vous et votre crush ğŸ˜ Ã©tant occupÃ©s Ã  vÃ©rifier que personne ne passe devant vous prendre vos burgers au moment oÃ¹ ils arriveront ğŸ•™, vous ne pouvez pas vous prÃ©occuper de votre crush ğŸ˜.

C'est du travail Â« synchrone Â», vous Ãªtre Â« synchronisÃ©s Â» avec le serveur/cuisinier ğŸ‘¨â€ğŸ³. Vous devez attendre ğŸ•™ et Ãªtre prÃ©sent au moment exact oÃ¹ le serveur/cuisinier ğŸ‘¨â€ğŸ³ finira les burgers ğŸ” et vous les donnera, sinon quelqu'un risque de vous les prendre.

<img src="/img/async/parallel-burgers/parallel-burgers-04.png" class="illustration">

Puis le serveur/cuisinier ğŸ‘¨â€ğŸ³ revient enfin avec vos burgers ğŸ”, aprÃ¨s un long moment d'attente ğŸ•™ devant le comptoir.

<img src="/img/async/parallel-burgers/parallel-burgers-05.png" class="illustration">

Vous prenez vos burgers ğŸ” et allez Ã  une table avec votre crush ğŸ˜

Vous les mangez, et vous avez terminÃ© ğŸ” â¹.

<img src="/img/async/parallel-burgers/parallel-burgers-06.png" class="illustration">

Durant tout ce processus, il n'y a presque pas eu de discussions ou de flirts car la plupart de votre temps Ã  Ã©tÃ© passÃ© Ã  attendre ğŸ•™ devant le comptoir ğŸ˜.

/// info

Illustrations proposÃ©es par <a href="https://www.instagram.com/ketrinadrawsalot" class="external-link" target="_blank">Ketrina Thompson</a>. ğŸ¨

///

---

Dans ce scÃ©nario de burgers parallÃ¨les, vous Ãªtes un ordinateur / programme ğŸ¤– avec deux processeurs (vous et votre crush ğŸ˜) attendant ğŸ•™ Ã  deux et dÃ©diant votre attention â¯ Ã  Â« attendre devant le comptoir Â» ğŸ•™ pour une longue durÃ©e.

Le fast-food a 8 processeurs (serveurs/cuisiniers) ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³. Alors que le fast-food de burgers concurrents en avait 2 (un serveur et un cuisinier).

Et pourtant l'expÃ©rience finale n'est pas meilleure ğŸ˜.

---

C'est donc l'histoire Ã©quivalente parallÃ¨le pour les burgers ğŸ”.

Pour un exemple plus courant dans la Â« vie rÃ©elle Â», imaginez une banque.

Jusqu'Ã  rÃ©cemment, la plupart des banques avaient plusieurs caisses (et banquiers) ğŸ‘¨â€ğŸ’¼ğŸ‘¨â€ğŸ’¼ğŸ‘¨â€ğŸ’¼ğŸ‘¨â€ğŸ’¼ et une unique file d'attente ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™ğŸ•™.

Tous les banquiers faisaient l'intÃ©gralitÃ© du travail avec chaque client avant de passer au suivant ğŸ‘¨â€ğŸ’¼â¯.

Et vous deviez attendre ğŸ•™ dans la file pendant un long moment ou vous perdiez votre place.

Vous n'auriez donc probablement pas envie d'amener votre crush ğŸ˜ avec vous Ã  la banque ğŸ¦.

### Conclusion sur les burgers { #burger-conclusion }

Dans ce scÃ©nario des Â« burgers du fast-food avec votre crush Â», comme il y a beaucoup d'attente ğŸ•™, il est trÃ¨s logique d'avoir un systÃ¨me concurrent â¸ğŸ”€â¯.

Et c'est le cas pour la plupart des applications web.

Vous aurez de nombreux, nombreux utilisateurs, mais votre serveur attendra ğŸ•™ que leur connexion peu performante envoie des requÃªtes.

Puis vous attendrez ğŸ•™ de nouveau que leurs rÃ©ponses reviennent.

Cette Â« attente Â» ğŸ•™ se mesure en microsecondes, mais tout de mÃªme, en cumulÃ© cela fait beaucoup d'attente.

C'est pourquoi il est logique d'utiliser du code asynchrone â¸ğŸ”€â¯ pour des APIs web.

Ce type d'asynchronicitÃ© est ce qui a rendu NodeJS populaire (bien que NodeJS ne soit pas parallÃ¨le) et c'est la force de Go en tant que langage de programmation.

Et c'est le mÃªme niveau de performance que celui obtenu avec **FastAPI**.

Et comme on peut avoir du parallÃ©lisme et de l'asynchronicitÃ© en mÃªme temps, on obtient des performances plus hautes que la plupart des frameworks NodeJS testÃ©s et Ã©gales Ã  celles du Go, qui est un langage compilÃ© plus proche du C <a href="https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=query&l=zijmkf-1" class="external-link" target="_blank">(tout Ã§a grÃ¢ce Ã  Starlette)</a>.

### Est-ce que la concurrence est mieux que le parallÃ©lisme ? { #is-concurrency-better-than-parallelism }

Nope ! C'est Ã§a la morale de l'histoire.

La concurrence est diffÃ©rente du parallÃ©lisme. C'est mieux sur des scÃ©narios **spÃ©cifiques** qui impliquent beaucoup d'attente. Ã€ cause de Ã§a, c'est gÃ©nÃ©ralement bien meilleur que le parallÃ©lisme pour le dÃ©veloppement d'applications web. Mais pas pour tout.

Donc pour Ã©quilibrer tout Ã§a, imaginez l'histoire suivante :

> Vous devez nettoyer une grande et sale maison.

*Oui, c'est toute l'histoire*.

---

Il n'y a plus d'attente ğŸ•™ nulle part, juste beaucoup de travail Ã  effectuer, dans diffÃ©rentes piÃ¨ces de la maison.

Vous pourriez diviser en diffÃ©rentes sections comme avec les burgers, d'abord le salon, puis la cuisine, etc. Mais vous n'attendez ğŸ•™ rien, vous ne faites que nettoyer et nettoyer, la sÃ©paration en sections ne changerait rien au final.

Cela prendrait autant de temps pour finir avec ou sans sections (concurrence) et vous auriez effectuÃ© la mÃªme quantitÃ© de travail.

Mais dans ce cas, si pouviez amener 8 ex-serveurs/cuisiniers/devenus-nettoyeurs ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³ğŸ‘¨â€ğŸ³, et que chacun d'eux (plus vous) pouvait prendre une zone de la maison pour la nettoyer, vous pourriez faire tout le travail en parallÃ¨le, et finir plus tÃ´t.

Dans ce scÃ©nario, chacun des nettoyeurs (vous y compris) serait un processeur, faisant sa partie du travail.

Et comme la plupart du temps d'exÃ©cution est pris par du Â« vrai Â» travail (et non de l'attente), et que le travail dans un ordinateur est fait par un <abbr title="Central Processing Unit - UnitÃ© centrale de traitement">CPU</abbr>, ce sont des problÃ¨mes dits Â« CPU bound Â».

---

Des exemples communs d'opÃ©rations Â« CPU bound Â» sont les procÃ©dÃ©s qui requiÃ¨rent des traitements mathÃ©matiques complexes.

Par exemple :

* Traitements d'**audio** et d'**images**.
* La **vision par ordinateur** : une image est composÃ©e de millions de pixels, chaque pixel ayant 3 valeurs / couleurs, les traiter tous va nÃ©cessiter d'effectuer des traitements sur chaque pixel, et de prÃ©fÃ©rence tous en mÃªme temps.
* L'apprentissage automatique (ou **Machine Learning**) : cela nÃ©cessite de nombreuses multiplications de matrices et vecteurs. Imaginez une Ã©norme feuille de calcul remplie de nombres que vous multiplierez entre eux tous au mÃªme moment.
* L'apprentissage profond (ou **Deep Learning**) : est un sous-domaine du **Machine Learning**, donc les mÃªmes raisons s'appliquent. Avec la diffÃ©rence qu'il n'y a pas une unique feuille de calcul de nombres Ã  multiplier, mais une Ã©norme quantitÃ© d'entre elles, et dans de nombreux cas, on utilise un processeur spÃ©cial pour construire et / ou utiliser ces modÃ¨les.

### Concurrence + ParallÃ©lisme : Web + Machine Learning { #concurrency-parallelism-web-machine-learning }

Avec **FastAPI** vous pouvez bÃ©nÃ©ficier de la concurrence qui est trÃ¨s courante en dÃ©veloppement web (c'est l'attrait principal de NodeJS).

Mais vous pouvez aussi profiter du parallÃ©lisme et du multiprocessing (plusieurs processus s'exÃ©cutant en parallÃ¨le) afin de gÃ©rer des charges **CPU bound** qui sont rÃ©currentes dans les systÃ¨mes de *Machine Learning*.

Ã‡a, ajoutÃ© au fait que Python soit le langage le plus populaire pour la **Data Science**, le **Machine Learning** et surtout le **Deep Learning**, font de **FastAPI** un trÃ¨s bon choix pour les APIs et applications de **Data Science** / **Machine Learning**.

Pour comprendre comment mettre en place ce parallÃ©lisme en production, allez lire la section [DÃ©ploiement](deployment/index.md){.internal-link target=_blank}.

## `async` et `await` { #async-and-await }

Les versions modernes de Python ont une maniÃ¨re trÃ¨s intuitive de dÃ©finir le code asynchrone, tout en gardant une apparence de code Â« sÃ©quentiel Â» classique en laissant Python faire l'attente pour vous au bon moment.

Pour une opÃ©ration qui nÃ©cessite de l'attente avant de donner un rÃ©sultat et qui supporte ces nouvelles fonctionnalitÃ©s Python, vous pouvez l'utiliser comme tel :

```Python
burgers = await get_burgers(2)
```

Le mot-clÃ© important ici est `await`. Il informe Python qu'il faut attendre â¸ que `get_burgers(2)` finisse d'effectuer ses opÃ©rations ğŸ•™ avant de stocker les rÃ©sultats dans la variable `burgers`. GrÃ¢ce Ã  cela, Python saura qu'il peut aller effectuer d'autres opÃ©rations ğŸ”€ â¯ pendant ce temps (comme par exemple recevoir une autre requÃªte).

Pour que `await` fonctionne, il doit Ãªtre placÃ© dans une fonction qui supporte l'asynchronicitÃ©. Pour que Ã§a soit le cas, il faut dÃ©clarer cette derniÃ¨re avec `async def` :

```Python hl_lines="1"
async def get_burgers(number: int):
    # OpÃ©rations asynchrones pour crÃ©er les burgers
    return burgers
```

... et non `def` :

```Python hl_lines="2"
# Ceci n'est pas asynchrone
def get_sequential_burgers(number: int):
    # OpÃ©rations sÃ©quentielles pour crÃ©er les burgers
    return burgers
```

Avec `async def`, Python sait que dans cette fonction il doit prendre en compte les expressions `await`, et qu'il peut mettre en pause â¸ l'exÃ©cution de la fonction pour aller faire autre chose ğŸ”€ avant de revenir.

Pour appeler une fonction dÃ©finie avec `async def`, vous devez utiliser `await`. Donc ceci ne marche pas :

```Python
# Ceci ne fonctionne pas, car get_burgers a Ã©tÃ© dÃ©fini avec async def
burgers = get_burgers(2)
```

---

Donc, si vous utilisez une bibliothÃ¨que qui nÃ©cessite que ses fonctions soient appelÃ©es avec `await`, vous devez dÃ©finir la *fonction de chemin d'accÃ¨s* en utilisant `async def` comme dans :

```Python hl_lines="2-3"
@app.get('/burgers')
async def read_burgers():
    burgers = await get_burgers(2)
    return burgers
```

### Plus de dÃ©tails techniques { #more-technical-details }

Vous avez donc compris que `await` peut seulement Ãªtre utilisÃ© dans des fonctions dÃ©finies avec `async def`.

Mais en mÃªme temps, les fonctions dÃ©finies avec `async def` doivent Ãªtre appelÃ©es avec `await` et donc dans des fonctions dÃ©finies elles aussi avec `async def`.

Vous avez donc remarquÃ© ce paradoxe d'Å“uf et de la poule, comment appelle-t-on la premiÃ¨re fonction `async` ?

Si vous utilisez **FastAPI**, pas besoin de vous en inquiÃ©ter, car cette Â« premiÃ¨re Â» fonction sera votre *fonction de chemin d'accÃ¨s* ; et **FastAPI** saura comment arriver au rÃ©sultat attendu.

Mais si vous souhaitez utiliser `async` / `await` sans FastAPI, vous pouvez Ã©galement le faire.

### Ã‰crire votre propre code async { #write-your-own-async-code }

Starlette (et **FastAPI**) sâ€™appuie sur <a href="https://anyio.readthedocs.io/en/stable/" class="external-link" target="_blank">AnyIO</a>, ce qui le rend compatible Ã  la fois avec la bibliothÃ¨que standard <a href="https://docs.python.org/3/library/asyncio-task.html" class="external-link" target="_blank">asyncio</a> de Python et avec <a href="https://trio.readthedocs.io/en/stable/" class="external-link" target="_blank">Trio</a>.

En particulier, vous pouvez utiliser directement <a href="https://anyio.readthedocs.io/en/stable/" class="external-link" target="_blank">AnyIO</a> pour vos cas dâ€™usage de concurrence avancÃ©s qui nÃ©cessitent des schÃ©mas plus Ã©laborÃ©s dans votre propre code.

Et mÃªme si vous nâ€™utilisiez pas FastAPI, vous pourriez aussi Ã©crire vos propres applications async avec <a href="https://anyio.readthedocs.io/en/stable/" class="external-link" target="_blank">AnyIO</a> pour une grande compatibilitÃ© et pour bÃ©nÃ©ficier de ses avantages (par ex. la Â« structured concurrency Â»).

Jâ€™ai crÃ©Ã© une autre bibliothÃ¨que au-dessus dâ€™AnyIO, comme une fine surcouche, pour amÃ©liorer un peu les annotations de type et obtenir une meilleure **autocomplÃ©tion**, des **erreurs en ligne**, etc. Elle propose Ã©galement une introduction et un tutoriel accessibles pour vous aider Ã  **comprendre** et Ã©crire **votre propre code async** : <a href="https://asyncer.tiangolo.com/" class="external-link" target="_blank">Asyncer</a>. Elle sera particuliÃ¨rement utile si vous devez **combiner du code async avec du code classique** (bloquant/synchrone).

### Autres formes de code asynchrone { #other-forms-of-asynchronous-code }

L'utilisation d'`async` et `await` est relativement nouvelle dans ce langage.

Mais cela rend la programmation asynchrone bien plus simple.

Cette mÃªme syntaxe (ou presque) a aussi Ã©tÃ© incluse rÃ©cemment dans les versions modernes de JavaScript (dans les navigateurs et NodeJS).

Mais avant Ã§a, gÃ©rer du code asynchrone Ã©tait bien plus complexe et difficile.

Dans les versions prÃ©cÃ©dentes de Python, vous auriez utilisÃ© des threads ou <a href="https://www.gevent.org/" class="external-link" target="_blank">Gevent</a>.  Mais le code aurait Ã©tÃ© bien plus difficile Ã  comprendre, dÃ©bugger, et concevoir.

Dans les versions prÃ©cÃ©dentes de JavaScript cÃ´tÃ© navigateur / NodeJS, vous auriez utilisÃ© des Â« callbacks Â». Menant potentiellement Ã  ce que l'on appelle le Â« callback hell Â».

## Coroutines { #coroutines }

Â« Coroutine Â» est juste un terme Ã©laborÃ© pour dÃ©signer ce qui est retournÃ© par une fonction dÃ©finie avec `async def`. Python sait que c'est comme une fonction classique qui va dÃ©marrer Ã  un moment et terminer Ã  un autre, mais qu'elle peut aussi Ãªtre mise en pause â¸, du moment qu'il y a un `await` dans son contenu.

Mais toutes ces fonctionnalitÃ©s d'utilisation de code asynchrone avec `async` et `await` sont souvent rÃ©sumÃ©es comme l'utilisation des Â« coroutines Â». On peut comparer cela Ã  la principale fonctionnalitÃ© clÃ© de Go, les Â« Goroutines Â».

## Conclusion { #conclusion }

Reprenons la phrase du dÃ©but de la page :

> Les versions modernes de Python supportent le **code asynchrone** grÃ¢ce aux **Â« coroutines Â»** avec les syntaxes **`async` et `await`**.

Ceci devrait Ãªtre plus comprÃ©hensible dÃ©sormais. âœ¨

Tout ceci est donc ce qui donne sa force Ã  FastAPI (Ã  travers Starlette) et lui permet d'avoir une performance aussi impressionnante.

## DÃ©tails trÃ¨s techniques { #very-technical-details }

/// warning | Alertes

Vous pouvez probablement ignorer cela.

Ce sont des dÃ©tails trÃ¨s poussÃ©s sur comment **FastAPI** fonctionne en arriÃ¨re-plan.

Si vous avez de bonnes connaissances techniques (coroutines, threads, code bloquant, etc.) et Ãªtes curieux de comment **FastAPI** gÃ¨re `async def` versus le `def` classique, cette partie est faite pour vous.

///

### Fonctions de chemin d'accÃ¨s { #path-operation-functions }

Quand vous dÃ©clarez une *fonction de chemin d'accÃ¨s* avec un `def` normal et non `async def`, elle est exÃ©cutÃ©e dans un groupe de threads (threadpool) externe qui est ensuite attendu, plutÃ´t que d'Ãªtre appelÃ©e directement (car cela bloquerait le serveur).

Si vous venez d'un autre framework asynchrone qui ne fonctionne pas comme de la faÃ§on dÃ©crite ci-dessus et que vous Ãªtes habituÃ© Ã  dÃ©finir des *fonctions de chemin d'accÃ¨s* basiques et purement calculatoires avec un simple `def` pour un faible gain de performance (environ 100 nanosecondes), veuillez noter que dans **FastAPI**, l'effet serait plutÃ´t contraire. Dans ces cas-lÃ , il vaut mieux utiliser `async def` Ã  moins que votre *fonction de chemin d'accÃ¨s* utilise du code qui effectue des opÃ©rations <abbr title="Input/Output - EntrÃ©es/Sorties: lecture ou Ã©criture sur le disque, communications rÃ©seau.">I/O</abbr> bloquantes.

Au final, dans les deux situations, il est fort probable que **FastAPI** soit tout de mÃªme [plus rapide](index.md#performance){.internal-link target=_blank} que (ou au moins de vitesse Ã©gale Ã ) votre framework prÃ©cÃ©dent.

### DÃ©pendances { #dependencies }

La mÃªme chose s'applique aux [dÃ©pendances](tutorial/dependencies/index.md){.internal-link target=_blank}. Si une dÃ©pendance est dÃ©finie avec `def` plutÃ´t que `async def`, elle est exÃ©cutÃ©e dans la threadpool externe.

### Sous-dÃ©pendances { #sub-dependencies }

Vous pouvez avoir de multiples dÃ©pendances et [sous-dÃ©pendances](tutorial/dependencies/sub-dependencies.md){.internal-link target=_blank} dÃ©pendant les unes des autres (en tant que paramÃ¨tres de la dÃ©finition de la *fonction de chemin d'accÃ¨s*), certaines crÃ©Ã©es avec `async def` et d'autres avec `def`. Cela fonctionnerait aussi, et celles dÃ©finies avec un simple `def` seraient exÃ©cutÃ©es sur un thread externe (venant de la threadpool) plutÃ´t que d'Ãªtre Â« attendues Â».

### Autres fonctions utilitaires { #other-utility-functions }

Toute autre fonction utilitaire que vous appelez directement peut Ãªtre crÃ©Ã©e avec un classique `def` ou avec `async def` et FastAPI n'aura pas d'impact sur la faÃ§on dont vous l'appelez.

Contrairement aux fonctions que FastAPI appelle pour vous : les *fonctions de chemin d'accÃ¨s* et dÃ©pendances.

Si votre fonction utilitaire est une fonction classique dÃ©finie avec `def`, elle sera appelÃ©e directement (telle qu'Ã©crite dans votre code), pas dans une threadpool ; si la fonction est dÃ©finie avec `async def` alors vous devrez attendre (avec `await`) que cette fonction se termine avant de passer Ã  la suite du code.

---

Encore une fois, ce sont des dÃ©tails trÃ¨s techniques qui peuvent Ãªtre utiles si vous venez ici les chercher.

Sinon, les instructions de la section <a href="#in-a-hurry">Vous Ãªtes pressÃ©s ?</a> ci-dessus sont largement suffisantes.
